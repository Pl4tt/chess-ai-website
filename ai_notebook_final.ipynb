{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7jSpi3P74fZS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import re\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3FttgiyJ4kHO"
      },
      "outputs": [],
      "source": [
        "piece_mappings = {\n",
        "    \"P\": 1,\n",
        "    \"N\": 2,\n",
        "    \"B\": 3,\n",
        "    \"R\": 4,\n",
        "    \"Q\": 5,\n",
        "    \"K\": 6,\n",
        "    \"p\": -1,\n",
        "    \"n\": -2,\n",
        "    \"b\": -3,\n",
        "    \"r\": -4,\n",
        "    \"q\": -5,\n",
        "    \"k\": -6,\n",
        "}\n",
        "board_mappings = {\n",
        "    \"a\": 0,\n",
        "    \"b\": 8,\n",
        "    \"c\": 16,\n",
        "    \"d\": 24,\n",
        "    \"e\": 32,\n",
        "    \"f\": 40,\n",
        "    \"g\": 48,\n",
        "    \"h\": 56\n",
        "}\n",
        "BATCH_SIZE = 256\n",
        "BUFFER_SIZE = 20000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "seISLLVW4pHo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_values(epochs, acc):\n",
        "\n",
        "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "    plt.title('Training accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    plt.savefig('/content/drive/MyDrive/accuracy.png')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def create_model(num_hneurons, model_optimizer):\n",
        "    model = keras.Sequential()\n",
        "    num_hlayers = len(num_hneurons)\n",
        "    input_shape = (70,)\n",
        "\n",
        "    model.add(layers.InputLayer(input_shape=input_shape))\n",
        "\n",
        "    for i in range(1, num_hlayers):\n",
        "        model.add(layers.Dense(num_hneurons[i], activation=\"relu\"))\n",
        "\n",
        "    model.add(layers.Dense(1, activation=\"linear\"))\n",
        "\n",
        "    model.compile(optimizer=model_optimizer, loss=\"mean_squared_error\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "@tf.function\n",
        "def train_step(data):\n",
        "    x, y = data\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(x, training=True)  # Forward pass\n",
        "        loss = model.compute_loss(y=y, y_pred=y_pred)\n",
        "\n",
        "    trainable_vars = model.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "    model.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "    for metric in model.metrics:\n",
        "        if metric.name == \"loss\":\n",
        "            metric.update_state(loss)\n",
        "        else:\n",
        "            metric.update_state(y, y_pred)\n",
        "\n",
        "    return {m.name: m.result() for m in model.metrics}\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"start Epoch {epoch}\")\n",
        "        start = time.time()\n",
        "\n",
        "        for data_batch in dataset:\n",
        "            train_step(data_batch)\n",
        "\n",
        "        if (epoch+1)%10 == 0:\n",
        "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "        print(model.evaluate(test_inp, test_outp))\n",
        "        print(f\"Epoch {epoch} finished in {time.time()-start}\")\n",
        "\n",
        "def fit_model(model, dataset):\n",
        "    epochs = 200\n",
        "\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        print(0)\n",
        "        history = model.fit(dataset, epochs=epochs)\n",
        "        # history = model.fit(dataset)\n",
        "        # train(dataset, epochs)\n",
        "\n",
        "    # result = model.evaluate(input_data[0:100], output_data[0:100])\n",
        "\n",
        "    # print(f\"result: {result}\")\n",
        "    print(\"Done\")\n",
        "    plot_values(epochs, history.history[\"accuracy\"])\n",
        "\n",
        "def process_data(json_data):\n",
        "    clean_input = []\n",
        "    clean_output = []\n",
        "\n",
        "    for position in json_data[\"data\"]:\n",
        "        fen = position[\"fen\"].split()\n",
        "        player = 1 if fen[1] == \"w\" else -1  # 1 for white, -1 for black\n",
        "\n",
        "        best_evaluation = -player*float(\"inf\")  # initially worst evaluation for current player\n",
        "\n",
        "        for line in position[\"evals\"][0][\"pvs\"]:\n",
        "            # if color*best_evaluation == float(\"inf\"):\n",
        "            #     continue\n",
        "\n",
        "            cp = line.get(\"cp\")\n",
        "            mate = line.get(\"mate\")\n",
        "\n",
        "            if cp is None:\n",
        "                if mate is not None and player*mate > 0:\n",
        "                    best_evaluation = player*float(\"inf\")\n",
        "            elif player == 1 and cp > best_evaluation or player == -1 and best_evaluation > cp:\n",
        "                best_evaluation = cp\n",
        "\n",
        "        matrix_fen = []\n",
        "\n",
        "\n",
        "        for square in fen[0].replace(\"/\", \"\"):\n",
        "            if square in piece_mappings.keys():\n",
        "                matrix_fen.append(piece_mappings.get(square))\n",
        "            else:\n",
        "                matrix_fen += [0]*int(square)\n",
        "\n",
        "        matrix_fen += [\n",
        "            player,\n",
        "            int(\"K\" in fen[2]),\n",
        "            int(\"Q\" in fen[2]),\n",
        "            int(\"k\" in fen[2]),\n",
        "            int(\"q\" in fen[2]),\n",
        "            -1 if fen[3] == \"-\" else board_mappings[fen[3][0]] + int(fen[3][1])-1,\n",
        "        ]\n",
        "\n",
        "        clean_input.append(matrix_fen)\n",
        "\n",
        "        if best_evaluation == +float(\"inf\"):\n",
        "            clean_output.append(1000000)\n",
        "        elif best_evaluation == -float(\"inf\"):\n",
        "            clean_output.append(-1000000)\n",
        "        else:\n",
        "            clean_output.append(best_evaluation)\n",
        "\n",
        "    return clean_input, clean_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W68og555-pX7",
        "outputId": "07b17157-f1e9-4f30-c148-9b198da6b467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBjLsbUc4ww_",
        "outputId": "0c7f1137-77fd-40a1-c060-6c17cbc9af4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "<_BatchDataset element_spec=(TensorSpec(shape=(None, 70), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "drive_filepath = \"/content/drive/MyDrive/usable_lichess_db_eval1.json\"\n",
        "with open(drive_filepath) as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "\n",
        "input_data, output_data = process_data(json_data)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_data, output_data)).batch(BATCH_SIZE)\n",
        "\n",
        "del json_data\n",
        "del input_data\n",
        "del output_data\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cLvHm2Ji4_Ho"
      },
      "outputs": [],
      "source": [
        "# optimizer = keras.optimizers.Adam(1e-4)\n",
        "# model = create_model([70, 128, 32], optimizer)\n",
        "\n",
        "checkpoint_dir = \"/content/drive/MyDrive/checkpoints2\"\n",
        "checkpoint_prefix = f\"{checkpoint_dir}/ckpt\"\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
        "\n",
        "test_inp, test_outp = next(iter(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEjjGVXq5K1q",
        "outputId": "256ab80a-8142-490e-e91c-ff3de544c7ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start Epoch 0\n",
            "Epoch 0 finished in 44.8333101272583\n",
            "start Epoch 1\n",
            "Epoch 1 finished in 41.7439603805542\n",
            "start Epoch 2\n",
            "Epoch 2 finished in 41.789498805999756\n",
            "start Epoch 3\n",
            "Epoch 3 finished in 41.546268701553345\n",
            "start Epoch 4\n",
            "Epoch 4 finished in 41.95014762878418\n",
            "start Epoch 5\n",
            "Epoch 5 finished in 41.56672692298889\n",
            "start Epoch 6\n",
            "Epoch 6 finished in 41.608290672302246\n",
            "start Epoch 7\n",
            "Epoch 7 finished in 41.375091791152954\n",
            "start Epoch 8\n",
            "Epoch 8 finished in 41.50683784484863\n",
            "start Epoch 9\n",
            "Epoch 9 finished in 41.83117604255676\n",
            "start Epoch 10\n",
            "Epoch 10 finished in 41.654293060302734\n",
            "start Epoch 11\n",
            "Epoch 11 finished in 41.566831827163696\n",
            "start Epoch 12\n",
            "Epoch 12 finished in 41.56121850013733\n",
            "start Epoch 13\n",
            "Epoch 13 finished in 41.40241861343384\n",
            "start Epoch 14\n",
            "Epoch 14 finished in 41.49090528488159\n",
            "start Epoch 15\n",
            "Epoch 15 finished in 41.677581787109375\n",
            "start Epoch 16\n",
            "Epoch 16 finished in 41.53267788887024\n",
            "start Epoch 17\n",
            "Epoch 17 finished in 41.588470697402954\n",
            "start Epoch 18\n",
            "Epoch 18 finished in 41.24148225784302\n",
            "start Epoch 19\n",
            "Epoch 19 finished in 41.699573278427124\n",
            "start Epoch 20\n",
            "Epoch 20 finished in 41.16489052772522\n",
            "start Epoch 21\n",
            "Epoch 21 finished in 41.20197892189026\n",
            "start Epoch 22\n",
            "Epoch 22 finished in 41.18881940841675\n",
            "start Epoch 23\n",
            "Epoch 23 finished in 41.00660419464111\n",
            "start Epoch 24\n",
            "Epoch 24 finished in 41.41434121131897\n",
            "start Epoch 25\n",
            "Epoch 25 finished in 41.12328624725342\n",
            "start Epoch 26\n",
            "Epoch 26 finished in 41.3230037689209\n",
            "start Epoch 27\n",
            "Epoch 27 finished in 41.1995644569397\n",
            "start Epoch 28\n",
            "Epoch 28 finished in 41.17658829689026\n",
            "start Epoch 29\n",
            "Epoch 29 finished in 41.572779178619385\n",
            "start Epoch 30\n",
            "Epoch 30 finished in 41.18801832199097\n",
            "start Epoch 31\n",
            "Epoch 31 finished in 41.09230637550354\n",
            "start Epoch 32\n",
            "Epoch 32 finished in 41.11364197731018\n",
            "start Epoch 33\n",
            "Epoch 33 finished in 41.26293420791626\n",
            "start Epoch 34\n",
            "Epoch 34 finished in 41.57467985153198\n",
            "start Epoch 35\n",
            "Epoch 35 finished in 41.1006498336792\n",
            "start Epoch 36\n",
            "Epoch 36 finished in 41.19214916229248\n",
            "start Epoch 37\n",
            "Epoch 37 finished in 41.283547163009644\n",
            "start Epoch 38\n",
            "Epoch 38 finished in 41.1908495426178\n",
            "start Epoch 39\n",
            "Epoch 39 finished in 41.17383003234863\n",
            "start Epoch 40\n",
            "Epoch 40 finished in 41.16718602180481\n",
            "start Epoch 41\n",
            "Epoch 41 finished in 41.61839032173157\n",
            "start Epoch 42\n",
            "Epoch 42 finished in 41.37796139717102\n",
            "start Epoch 43\n",
            "Epoch 43 finished in 41.29025459289551\n",
            "start Epoch 44\n",
            "Epoch 44 finished in 41.456674575805664\n",
            "start Epoch 45\n",
            "Epoch 45 finished in 41.26532173156738\n",
            "start Epoch 46\n",
            "Epoch 46 finished in 41.40987706184387\n",
            "start Epoch 47\n",
            "Epoch 47 finished in 41.18382787704468\n",
            "start Epoch 48\n",
            "Epoch 48 finished in 41.28188443183899\n",
            "start Epoch 49\n",
            "Epoch 49 finished in 41.9676673412323\n",
            "start Epoch 50\n",
            "Epoch 50 finished in 41.28331899642944\n",
            "start Epoch 51\n",
            "Epoch 51 finished in 41.22409677505493\n",
            "start Epoch 52\n",
            "Epoch 52 finished in 41.2249698638916\n",
            "start Epoch 53\n",
            "Epoch 53 finished in 41.04782032966614\n",
            "start Epoch 54\n",
            "Epoch 54 finished in 41.490755558013916\n",
            "start Epoch 55\n",
            "Epoch 55 finished in 41.31395697593689\n",
            "start Epoch 56\n",
            "Epoch 56 finished in 41.60586357116699\n",
            "start Epoch 57\n",
            "Epoch 57 finished in 41.35187888145447\n",
            "start Epoch 58\n",
            "Epoch 58 finished in 41.130295276641846\n",
            "start Epoch 59\n",
            "Epoch 59 finished in 41.233166456222534\n",
            "start Epoch 60\n",
            "Epoch 60 finished in 41.31119346618652\n",
            "start Epoch 61\n",
            "Epoch 61 finished in 41.25736856460571\n",
            "start Epoch 62\n",
            "Epoch 62 finished in 41.33274745941162\n",
            "start Epoch 63\n",
            "Epoch 63 finished in 41.16309595108032\n",
            "start Epoch 64\n",
            "Epoch 64 finished in 41.542299032211304\n",
            "start Epoch 65\n",
            "Epoch 65 finished in 41.16283082962036\n",
            "start Epoch 66\n",
            "Epoch 66 finished in 41.407981872558594\n",
            "start Epoch 67\n",
            "Epoch 67 finished in 41.14762043952942\n",
            "start Epoch 68\n",
            "Epoch 68 finished in 41.10444402694702\n",
            "start Epoch 69\n",
            "Epoch 69 finished in 41.397440671920776\n",
            "start Epoch 70\n",
            "Epoch 70 finished in 41.18722939491272\n",
            "start Epoch 71\n",
            "Epoch 71 finished in 41.63810920715332\n",
            "start Epoch 72\n",
            "Epoch 72 finished in 41.3051438331604\n",
            "start Epoch 73\n",
            "Epoch 73 finished in 41.26749134063721\n",
            "start Epoch 74\n",
            "Epoch 74 finished in 41.40411448478699\n",
            "start Epoch 75\n",
            "Epoch 75 finished in 41.389551401138306\n",
            "start Epoch 76\n",
            "Epoch 76 finished in 41.335718870162964\n",
            "start Epoch 77\n",
            "Epoch 77 finished in 41.219950675964355\n",
            "start Epoch 78\n",
            "Epoch 78 finished in 41.313244342803955\n",
            "start Epoch 79\n",
            "Epoch 79 finished in 41.54703450202942\n",
            "start Epoch 80\n",
            "Epoch 80 finished in 41.41147470474243\n",
            "start Epoch 81\n",
            "Epoch 81 finished in 41.12773299217224\n",
            "start Epoch 82\n",
            "Epoch 82 finished in 41.33757567405701\n",
            "start Epoch 83\n",
            "Epoch 83 finished in 41.31795048713684\n",
            "start Epoch 84\n",
            "Epoch 84 finished in 41.429166078567505\n",
            "start Epoch 85\n",
            "Epoch 85 finished in 41.1215283870697\n",
            "start Epoch 86\n",
            "Epoch 86 finished in 41.545114517211914\n",
            "start Epoch 87\n",
            "Epoch 87 finished in 41.24376726150513\n",
            "start Epoch 88\n",
            "Epoch 88 finished in 41.2929208278656\n",
            "start Epoch 89\n",
            "Epoch 89 finished in 41.36463665962219\n",
            "start Epoch 90\n",
            "Epoch 90 finished in 41.311569690704346\n",
            "start Epoch 91\n",
            "Epoch 91 finished in 41.3707811832428\n",
            "start Epoch 92\n",
            "Epoch 92 finished in 41.360275983810425\n",
            "start Epoch 93\n",
            "Epoch 93 finished in 41.424994230270386\n",
            "start Epoch 94\n",
            "Epoch 94 finished in 41.35819721221924\n",
            "start Epoch 95\n",
            "Epoch 95 finished in 41.302457094192505\n",
            "start Epoch 96\n",
            "Epoch 96 finished in 41.27080988883972\n",
            "start Epoch 97\n",
            "Epoch 97 finished in 41.33411478996277\n",
            "start Epoch 98\n",
            "Epoch 98 finished in 41.301817178726196\n",
            "start Epoch 99\n",
            "Epoch 99 finished in 41.520551919937134\n"
          ]
        }
      ],
      "source": [
        "train(dataset, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FPCJNrCIbYB",
        "outputId": "0f37aedb-9bd0-4c63-f6c2-2b35ecbbaf6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save(\"/content/drive/MyDrive/model_300.h5\")\n",
        "model.save(\"/content/drive/MyDrive/model_300.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GlVrsFQTzoO",
        "outputId": "1115a0f4-90db-4485-a209-d01a9a67da46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-4 -2 -3 ...  1  1 -1]\n",
            " [-4 -2 -3 ...  1  1 -1]\n",
            " [ 0  0  4 ...  0  0 -1]\n",
            " ...\n",
            " [-4 -2  0 ...  1  1 -1]\n",
            " [-4 -2 -3 ...  1  1 -1]\n",
            " [-4 -2 -3 ...  1  1 -1]], shape=(256, 70), dtype=int32) tf.Tensor(\n",
            "[      20       64        0        0      994       20       31        0\n",
            "       13        0       60     -131     -767      -64       58     -815\n",
            "      -94      498       35       39       31       53       59       12\n",
            "      778      483       11       39     -116       41      148       21\n",
            "       33       -6        0     -740       87        0       69       31\n",
            "  1000000       79      789       42       39        0       64       10\n",
            "       75      789       72      -21    -6180      728 -1000000      627\n",
            "       -8     -701       58      144       36       32       96      516\n",
            "      994     -723      724       39       14      379       34      -21\n",
            "     -700       49      616      -86      -37        0      201      -11\n",
            "       98      728     1109      137       75       73       42       78\n",
            "    -1721       16       56       14       21        0        0        0\n",
            "  1000000      -20        0       46       25       19       64       53\n",
            "       15        0       33      401       25      747        0       16\n",
            "       78       64       20       42       37       14       31     -332\n",
            "      -35       40       70      -66       67       88      -48       50\n",
            "      779        0       12       17       67       59     4466        3\n",
            "      -67       13       16       13     -164       87     -835       71\n",
            "       13       45       27  1000000  1000000       20        0       31\n",
            "       43     -905      -56        0      -58       47     -188  1000000\n",
            "      -17        0        0  1000000      -94        0       48      -16\n",
            "  1000000        0  1000000        0       62      -67      -12       58\n",
            "      -23       24       41       17       44       83      -85       25\n",
            "     -562       69     -194        0       36       66       62       75\n",
            " -1000000       46       66     -325  1000000       52       20       75\n",
            "       67  1000000       73       36       19       25      -74       65\n",
            "       71       13      -26       40       64 -1000000       74       33\n",
            "       94       80       62       26  1000000       91     -744       68\n",
            " -1000000       42      569  1000000       49      151      571       55\n",
            "      145       36        0      640      661     -344     -259      510\n",
            "      557       87       26      480      676       43     -530       95\n",
            "       24        0     -480     5438       60       20       -9       67], shape=(256,), dtype=int32)\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 49180925952.0000 - accuracy: 0.0273\n",
            "[49180925952.0, 0.02734375]\n",
            "<_BatchDataset element_spec=(TensorSpec(shape=(None, 70), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "test_inp, test_outp = next(iter(dataset))\n",
        "print(test_inp, test_outp)\n",
        "print(model.evaluate(test_inp, test_outp))\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LM8IBn5gW8RC",
        "outputId": "65b25f02-3a40-46a3-d08d-1f78ed8e7743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Epoch 1/200\n",
            "22673/22673 [==============================] - 61s 3ms/step - loss: 55939047424.0000 - accuracy: 0.0327\n",
            "Epoch 2/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 56172412928.0000 - accuracy: 0.0323\n",
            "Epoch 3/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 56124227584.0000 - accuracy: 0.0325\n",
            "Epoch 4/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 56057860096.0000 - accuracy: 0.0327\n",
            "Epoch 5/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 55981101056.0000 - accuracy: 0.0328\n",
            "Epoch 6/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 55894097920.0000 - accuracy: 0.0330\n",
            "Epoch 7/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 55796285440.0000 - accuracy: 0.0332\n",
            "Epoch 8/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 55691833344.0000 - accuracy: 0.0334\n",
            "Epoch 9/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 55582605312.0000 - accuracy: 0.0336\n",
            "Epoch 10/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 55470202880.0000 - accuracy: 0.0339\n",
            "Epoch 11/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 55353815040.0000 - accuracy: 0.0342\n",
            "Epoch 12/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 55232729088.0000 - accuracy: 0.0344\n",
            "Epoch 13/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 55107440640.0000 - accuracy: 0.0347\n",
            "Epoch 14/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 54980263936.0000 - accuracy: 0.0350\n",
            "Epoch 15/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 54852902912.0000 - accuracy: 0.0352\n",
            "Epoch 16/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 54726426624.0000 - accuracy: 0.0355\n",
            "Epoch 17/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 54600503296.0000 - accuracy: 0.0358\n",
            "Epoch 18/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 54476193792.0000 - accuracy: 0.0360\n",
            "Epoch 19/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 54355091456.0000 - accuracy: 0.0362\n",
            "Epoch 20/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 54237065216.0000 - accuracy: 0.0364\n",
            "Epoch 21/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 54122397696.0000 - accuracy: 0.0365\n",
            "Epoch 22/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 54011305984.0000 - accuracy: 0.0366\n",
            "Epoch 23/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 53901725696.0000 - accuracy: 0.0367\n",
            "Epoch 24/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 53789896704.0000 - accuracy: 0.0369\n",
            "Epoch 25/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 53679714304.0000 - accuracy: 0.0370\n",
            "Epoch 26/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 53575548928.0000 - accuracy: 0.0370\n",
            "Epoch 27/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 53476556800.0000 - accuracy: 0.0370\n",
            "Epoch 28/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 53383872512.0000 - accuracy: 0.0371\n",
            "Epoch 29/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 53296492544.0000 - accuracy: 0.0372\n",
            "Epoch 30/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 53213855744.0000 - accuracy: 0.0373\n",
            "Epoch 31/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 53135167488.0000 - accuracy: 0.0373\n",
            "Epoch 32/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 53060128768.0000 - accuracy: 0.0374\n",
            "Epoch 33/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52987580416.0000 - accuracy: 0.0374\n",
            "Epoch 34/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52916772864.0000 - accuracy: 0.0374\n",
            "Epoch 35/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52847300608.0000 - accuracy: 0.0375\n",
            "Epoch 36/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52779601920.0000 - accuracy: 0.0375\n",
            "Epoch 37/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52712235008.0000 - accuracy: 0.0374\n",
            "Epoch 38/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52645203968.0000 - accuracy: 0.0374\n",
            "Epoch 39/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52577865728.0000 - accuracy: 0.0374\n",
            "Epoch 40/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52509847552.0000 - accuracy: 0.0374\n",
            "Epoch 41/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52441980928.0000 - accuracy: 0.0373\n",
            "Epoch 42/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52374192128.0000 - accuracy: 0.0373\n",
            "Epoch 43/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52307038208.0000 - accuracy: 0.0373\n",
            "Epoch 44/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52240371712.0000 - accuracy: 0.0373\n",
            "Epoch 45/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52173799424.0000 - accuracy: 0.0373\n",
            "Epoch 46/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52108005376.0000 - accuracy: 0.0372\n",
            "Epoch 47/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 52042821632.0000 - accuracy: 0.0372\n",
            "Epoch 48/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51977838592.0000 - accuracy: 0.0372\n",
            "Epoch 49/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51914559488.0000 - accuracy: 0.0372\n",
            "Epoch 50/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51852963840.0000 - accuracy: 0.0371\n",
            "Epoch 51/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51792830464.0000 - accuracy: 0.0371\n",
            "Epoch 52/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51734003712.0000 - accuracy: 0.0371\n",
            "Epoch 53/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51676884992.0000 - accuracy: 0.0371\n",
            "Epoch 54/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51621306368.0000 - accuracy: 0.0370\n",
            "Epoch 55/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51567206400.0000 - accuracy: 0.0370\n",
            "Epoch 56/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51514789888.0000 - accuracy: 0.0370\n",
            "Epoch 57/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51464151040.0000 - accuracy: 0.0370\n",
            "Epoch 58/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51414704128.0000 - accuracy: 0.0370\n",
            "Epoch 59/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51367538688.0000 - accuracy: 0.0370\n",
            "Epoch 60/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51321597952.0000 - accuracy: 0.0370\n",
            "Epoch 61/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51277373440.0000 - accuracy: 0.0370\n",
            "Epoch 62/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51235475456.0000 - accuracy: 0.0370\n",
            "Epoch 63/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51194933248.0000 - accuracy: 0.0369\n",
            "Epoch 64/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51156074496.0000 - accuracy: 0.0369\n",
            "Epoch 65/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51119124480.0000 - accuracy: 0.0369\n",
            "Epoch 66/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51083870208.0000 - accuracy: 0.0369\n",
            "Epoch 67/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51050151936.0000 - accuracy: 0.0368\n",
            "Epoch 68/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 51017687040.0000 - accuracy: 0.0368\n",
            "Epoch 69/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50986725376.0000 - accuracy: 0.0368\n",
            "Epoch 70/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50956873728.0000 - accuracy: 0.0368\n",
            "Epoch 71/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50928168960.0000 - accuracy: 0.0368\n",
            "Epoch 72/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50900283392.0000 - accuracy: 0.0367\n",
            "Epoch 73/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50873462784.0000 - accuracy: 0.0367\n",
            "Epoch 74/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50847969280.0000 - accuracy: 0.0367\n",
            "Epoch 75/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50823290880.0000 - accuracy: 0.0367\n",
            "Epoch 76/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50799370240.0000 - accuracy: 0.0367\n",
            "Epoch 77/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50776444928.0000 - accuracy: 0.0367\n",
            "Epoch 78/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50754170880.0000 - accuracy: 0.0367\n",
            "Epoch 79/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50732969984.0000 - accuracy: 0.0367\n",
            "Epoch 80/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50712231936.0000 - accuracy: 0.0367\n",
            "Epoch 81/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50691858432.0000 - accuracy: 0.0367\n",
            "Epoch 82/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50672353280.0000 - accuracy: 0.0367\n",
            "Epoch 83/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50653581312.0000 - accuracy: 0.0367\n",
            "Epoch 84/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50634977280.0000 - accuracy: 0.0367\n",
            "Epoch 85/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50616999936.0000 - accuracy: 0.0367\n",
            "Epoch 86/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50599530496.0000 - accuracy: 0.0367\n",
            "Epoch 87/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50582450176.0000 - accuracy: 0.0367\n",
            "Epoch 88/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50565627904.0000 - accuracy: 0.0367\n",
            "Epoch 89/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50548944896.0000 - accuracy: 0.0367\n",
            "Epoch 90/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50533351424.0000 - accuracy: 0.0367\n",
            "Epoch 91/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50517397504.0000 - accuracy: 0.0367\n",
            "Epoch 92/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50501963776.0000 - accuracy: 0.0367\n",
            "Epoch 93/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50486693888.0000 - accuracy: 0.0367\n",
            "Epoch 94/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50471849984.0000 - accuracy: 0.0367\n",
            "Epoch 95/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50457223168.0000 - accuracy: 0.0367\n",
            "Epoch 96/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50442838016.0000 - accuracy: 0.0367\n",
            "Epoch 97/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50428669952.0000 - accuracy: 0.0367\n",
            "Epoch 98/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50414817280.0000 - accuracy: 0.0368\n",
            "Epoch 99/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50400985088.0000 - accuracy: 0.0368\n",
            "Epoch 100/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50387628032.0000 - accuracy: 0.0368\n",
            "Epoch 101/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50374254592.0000 - accuracy: 0.0367\n",
            "Epoch 102/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50361499648.0000 - accuracy: 0.0368\n",
            "Epoch 103/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50348617728.0000 - accuracy: 0.0367\n",
            "Epoch 104/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50335952896.0000 - accuracy: 0.0368\n",
            "Epoch 105/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50323542016.0000 - accuracy: 0.0368\n",
            "Epoch 106/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50311049216.0000 - accuracy: 0.0368\n",
            "Epoch 107/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50298802176.0000 - accuracy: 0.0368\n",
            "Epoch 108/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50286972928.0000 - accuracy: 0.0368\n",
            "Epoch 109/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50274914304.0000 - accuracy: 0.0368\n",
            "Epoch 110/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50263158784.0000 - accuracy: 0.0368\n",
            "Epoch 111/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50251407360.0000 - accuracy: 0.0368\n",
            "Epoch 112/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50239799296.0000 - accuracy: 0.0368\n",
            "Epoch 113/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50228338688.0000 - accuracy: 0.0368\n",
            "Epoch 114/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50216857600.0000 - accuracy: 0.0368\n",
            "Epoch 115/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50205556736.0000 - accuracy: 0.0368\n",
            "Epoch 116/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50194280448.0000 - accuracy: 0.0368\n",
            "Epoch 117/200\n",
            "22673/22673 [==============================] - 60s 3ms/step - loss: 50183131136.0000 - accuracy: 0.0368\n",
            "Epoch 118/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50172182528.0000 - accuracy: 0.0368\n",
            "Epoch 119/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50161168384.0000 - accuracy: 0.0368\n",
            "Epoch 120/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50149793792.0000 - accuracy: 0.0368\n",
            "Epoch 121/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50138681344.0000 - accuracy: 0.0368\n",
            "Epoch 122/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50127519744.0000 - accuracy: 0.0368\n",
            "Epoch 123/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50116268032.0000 - accuracy: 0.0368\n",
            "Epoch 124/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50105032704.0000 - accuracy: 0.0368\n",
            "Epoch 125/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50093817856.0000 - accuracy: 0.0368\n",
            "Epoch 126/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50082316288.0000 - accuracy: 0.0368\n",
            "Epoch 127/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50071040000.0000 - accuracy: 0.0368\n",
            "Epoch 128/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50059698176.0000 - accuracy: 0.0368\n",
            "Epoch 129/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50048266240.0000 - accuracy: 0.0368\n",
            "Epoch 130/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50036842496.0000 - accuracy: 0.0368\n",
            "Epoch 131/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50025422848.0000 - accuracy: 0.0368\n",
            "Epoch 132/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50013872128.0000 - accuracy: 0.0368\n",
            "Epoch 133/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 50002522112.0000 - accuracy: 0.0368\n",
            "Epoch 134/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49990901760.0000 - accuracy: 0.0368\n",
            "Epoch 135/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49979191296.0000 - accuracy: 0.0368\n",
            "Epoch 136/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49967288320.0000 - accuracy: 0.0367\n",
            "Epoch 137/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49955926016.0000 - accuracy: 0.0367\n",
            "Epoch 138/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49944264704.0000 - accuracy: 0.0367\n",
            "Epoch 139/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49932865536.0000 - accuracy: 0.0367\n",
            "Epoch 140/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49921298432.0000 - accuracy: 0.0366\n",
            "Epoch 141/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49909940224.0000 - accuracy: 0.0366\n",
            "Epoch 142/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49898663936.0000 - accuracy: 0.0366\n",
            "Epoch 143/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49887109120.0000 - accuracy: 0.0366\n",
            "Epoch 144/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49876037632.0000 - accuracy: 0.0366\n",
            "Epoch 145/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49864744960.0000 - accuracy: 0.0366\n",
            "Epoch 146/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49853751296.0000 - accuracy: 0.0366\n",
            "Epoch 147/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49842966528.0000 - accuracy: 0.0366\n",
            "Epoch 148/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49831759872.0000 - accuracy: 0.0366\n",
            "Epoch 149/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49821057024.0000 - accuracy: 0.0366\n",
            "Epoch 150/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49810210816.0000 - accuracy: 0.0366\n",
            "Epoch 151/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49799286784.0000 - accuracy: 0.0366\n",
            "Epoch 152/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49788645376.0000 - accuracy: 0.0366\n",
            "Epoch 153/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49778212864.0000 - accuracy: 0.0366\n",
            "Epoch 154/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49767559168.0000 - accuracy: 0.0366\n",
            "Epoch 155/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49757130752.0000 - accuracy: 0.0366\n",
            "Epoch 156/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49746718720.0000 - accuracy: 0.0366\n",
            "Epoch 157/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49736437760.0000 - accuracy: 0.0366\n",
            "Epoch 158/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49725952000.0000 - accuracy: 0.0366\n",
            "Epoch 159/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49715847168.0000 - accuracy: 0.0366\n",
            "Epoch 160/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49705943040.0000 - accuracy: 0.0366\n",
            "Epoch 161/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49696038912.0000 - accuracy: 0.0366\n",
            "Epoch 162/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49686237184.0000 - accuracy: 0.0366\n",
            "Epoch 163/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49676242944.0000 - accuracy: 0.0366\n",
            "Epoch 164/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49666285568.0000 - accuracy: 0.0366\n",
            "Epoch 165/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49656180736.0000 - accuracy: 0.0366\n",
            "Epoch 166/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49646632960.0000 - accuracy: 0.0366\n",
            "Epoch 167/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49637089280.0000 - accuracy: 0.0366\n",
            "Epoch 168/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49627574272.0000 - accuracy: 0.0366\n",
            "Epoch 169/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49618149376.0000 - accuracy: 0.0366\n",
            "Epoch 170/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49608540160.0000 - accuracy: 0.0366\n",
            "Epoch 171/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49599045632.0000 - accuracy: 0.0365\n",
            "Epoch 172/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49590059008.0000 - accuracy: 0.0365\n",
            "Epoch 173/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49580568576.0000 - accuracy: 0.0365\n",
            "Epoch 174/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49571418112.0000 - accuracy: 0.0365\n",
            "Epoch 175/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49562492928.0000 - accuracy: 0.0365\n",
            "Epoch 176/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49553453056.0000 - accuracy: 0.0365\n",
            "Epoch 177/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49544548352.0000 - accuracy: 0.0365\n",
            "Epoch 178/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49535451136.0000 - accuracy: 0.0365\n",
            "Epoch 179/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49526607872.0000 - accuracy: 0.0365\n",
            "Epoch 180/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49517699072.0000 - accuracy: 0.0365\n",
            "Epoch 181/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49508978688.0000 - accuracy: 0.0365\n",
            "Epoch 182/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49500131328.0000 - accuracy: 0.0365\n",
            "Epoch 183/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49491357696.0000 - accuracy: 0.0365\n",
            "Epoch 184/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49482883072.0000 - accuracy: 0.0365\n",
            "Epoch 185/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49474338816.0000 - accuracy: 0.0365\n",
            "Epoch 186/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49465712640.0000 - accuracy: 0.0365\n",
            "Epoch 187/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49457233920.0000 - accuracy: 0.0364\n",
            "Epoch 188/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49448685568.0000 - accuracy: 0.0365\n",
            "Epoch 189/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49440309248.0000 - accuracy: 0.0365\n",
            "Epoch 190/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49432203264.0000 - accuracy: 0.0364\n",
            "Epoch 191/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49423888384.0000 - accuracy: 0.0364\n",
            "Epoch 192/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49415712768.0000 - accuracy: 0.0364\n",
            "Epoch 193/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49407520768.0000 - accuracy: 0.0364\n",
            "Epoch 194/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49399463936.0000 - accuracy: 0.0364\n",
            "Epoch 195/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49391366144.0000 - accuracy: 0.0364\n",
            "Epoch 196/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49383231488.0000 - accuracy: 0.0364\n",
            "Epoch 197/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49375223808.0000 - accuracy: 0.0364\n",
            "Epoch 198/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49367756800.0000 - accuracy: 0.0364\n",
            "Epoch 199/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49359560704.0000 - accuracy: 0.0364\n",
            "Epoch 200/200\n",
            "22673/22673 [==============================] - 59s 3ms/step - loss: 49351847936.0000 - accuracy: 0.0364\n",
            "Done\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'acc'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-1ed70796d663>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-684bd8c8a719>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# print(f\"result: {result}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mplot_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'acc'"
          ]
        }
      ],
      "source": [
        "fit_model(model, dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
