EPOCH 1:
  batch 1000 loss: 0.1319514721218825
  batch 2000 loss: 0.10094590021673754
  batch 3000 loss: 0.09287944119917058
LOSS train 0.09287944119917058 valid 0.05995632378823454
new best vloss: 0.05995632378823454
EPOCH 2:
  batch 1000 loss: 0.08719823228979492
  batch 2000 loss: 0.08400602690324023
  batch 3000 loss: 0.08534386433756577
LOSS train 0.08534386433756577 valid 0.0662054913993946
EPOCH 3:
  batch 1000 loss: 0.08212339573315153
  batch 2000 loss: 0.08112071868614944
  batch 3000 loss: 0.07912839155411729
LOSS train 0.07912839155411729 valid 0.103663513295002
EPOCH 4:
  batch 1000 loss: 0.07854246289156612
  batch 2000 loss: 0.0782892174340512
  batch 3000 loss: 0.07729220486263651
LOSS train 0.07729220486263651 valid 0.08133898931531197
EPOCH 5:
  batch 1000 loss: 0.07677307413756729
  batch 2000 loss: 0.07672754590924608
  batch 3000 loss: 0.07550709201162412
LOSS train 0.07550709201162412 valid 0.0873468487856068
EPOCH 6:
  batch 1000 loss: 0.07602881712128316
  batch 2000 loss: 0.07467182143429928
  batch 3000 loss: 0.07519822623433235
LOSS train 0.07519822623433235 valid 0.08289105344083698
EPOCH 7:
  batch 1000 loss: 0.07490869787363234
  batch 2000 loss: 0.07223661914255125
  batch 3000 loss: 0.07342209885761443
LOSS train 0.07342209885761443 valid 0.07642564382844814
EPOCH 8:
  batch 1000 loss: 0.07343815549204023
  batch 2000 loss: 0.0747389078049178
  batch 3000 loss: 0.07188879650503678
LOSS train 0.07188879650503678 valid 0.10349901780462005
EPOCH 9:
  batch 1000 loss: 0.07168923344303424
  batch 2000 loss: 0.07192872369015274
  batch 3000 loss: 0.07118745930550764
LOSS train 0.07118745930550764 valid 0.03887933139028519
new best vloss: 0.03887933139028519
EPOCH 10:
  batch 1000 loss: 0.07104882145353904
  batch 2000 loss: 0.07122579144534213
  batch 3000 loss: 0.07078514319363181
LOSS train 0.07078514319363181 valid 0.028769983449774373
new best vloss: 0.028769983449774373
EPOCH 11:
  batch 1000 loss: 0.07050544643962837
  batch 2000 loss: 0.07089205591159221
  batch 3000 loss: 0.07046193142827344
LOSS train 0.07046193142827344 valid 0.05565090617837996
EPOCH 12:
  batch 1000 loss: 0.07005665673340157
  batch 2000 loss: 0.06904710650462104
  batch 3000 loss: 0.06952260364548081
LOSS train 0.06952260364548081 valid 0.05328667268775386
EPOCH 13:
  batch 1000 loss: 0.06941610800603606
  batch 2000 loss: 0.07033515907994646
  batch 3000 loss: 0.0695962916317843
LOSS train 0.0695962916317843 valid 0.06712581560201822
EPOCH 14:
  batch 1000 loss: 0.06802882031386547
  batch 2000 loss: 0.06799641801448685
  batch 3000 loss: 0.06870269247865947
LOSS train 0.06870269247865947 valid 0.05642624136919494
EPOCH 15:
  batch 1000 loss: 0.06737705224277547
  batch 2000 loss: 0.06857340596573647
  batch 3000 loss: 0.06741235834133195
LOSS train 0.06741235834133195 valid 0.0754751147047197
EPOCH 16:
  batch 1000 loss: 0.06807187378100972
  batch 2000 loss: 0.06937955514049662
  batch 3000 loss: 0.06839169773839374
LOSS train 0.06839169773839374 valid 0.06581693553598597
EPOCH 17:
  batch 1000 loss: 0.06713449549251152
  batch 2000 loss: 0.0681971024450832
  batch 3000 loss: 0.06711078476911211
LOSS train 0.06711078476911211 valid 0.059530923276089234
EPOCH 18:
  batch 1000 loss: 0.06663877988936315
  batch 2000 loss: 0.06679614464133203
  batch 3000 loss: 0.06702653430193475
LOSS train 0.06702653430193475 valid 0.04333840908895847
EPOCH 19:
  batch 1000 loss: 0.06708300575674568
  batch 2000 loss: 0.06669122372481766
  batch 3000 loss: 0.0672954878433954
LOSS train 0.0672954878433954 valid 0.0745058309570093
EPOCH 20:
  batch 1000 loss: 0.06681410174985088
  batch 2000 loss: 0.0669400145262231
  batch 3000 loss: 0.06725631071425077
LOSS train 0.06725631071425077 valid 0.07008400963485997
EPOCH 21:
  batch 1000 loss: 0.06560275762000715
  batch 2000 loss: 0.06717196548347762
  batch 3000 loss: 0.0670157865196498
LOSS train 0.0670157865196498 valid 0.06829117672093617
EPOCH 22:
  batch 1000 loss: 0.06569234652356667
  batch 2000 loss: 0.06622960813295137
  batch 3000 loss: 0.06634904303962327
LOSS train 0.06634904303962327 valid 0.1028473658815192
EPOCH 23:
  batch 1000 loss: 0.06522935444290129
  batch 2000 loss: 0.06644512060113304
  batch 3000 loss: 0.06569993974031378
LOSS train 0.06569993974031378 valid 0.07372120862495954
EPOCH 24:
  batch 1000 loss: 0.06611711145856444
  batch 2000 loss: 0.06562015580514649
  batch 3000 loss: 0.06607635662325756
LOSS train 0.06607635662325756 valid 0.08203463590273259
EPOCH 25:
  batch 1000 loss: 0.0654865882258013
  batch 2000 loss: 0.06337711184384485
  batch 3000 loss: 0.06512304689514725
LOSS train 0.06512304689514725 valid 0.06450413828858169
EPOCH 26:
  batch 1000 loss: 0.06697644936940943
  batch 2000 loss: 0.06607537851122307
  batch 3000 loss: 0.0649304162764113
LOSS train 0.0649304162764113 valid 0.078733767826904
EPOCH 27:
  batch 1000 loss: 0.06527827411137319
  batch 2000 loss: 0.06501156052464704
  batch 3000 loss: 0.06530101955259916
LOSS train 0.06530101955259916 valid 0.05255581974180975
EPOCH 28:
  batch 1000 loss: 0.06361956636393859
  batch 2000 loss: 0.06412224691247022
  batch 3000 loss: 0.06483879337204915
LOSS train 0.06483879337204915 valid 0.0499086421069857
EPOCH 29:
  batch 1000 loss: 0.06382297732570903
  batch 2000 loss: 0.06410518977364628
  batch 3000 loss: 0.06497919742873627
LOSS train 0.06497919742873627 valid 0.054459216156847866
EPOCH 30:
  batch 1000 loss: 0.06563351616497712
  batch 2000 loss: 0.06498750205676469
  batch 3000 loss: 0.06400345332994554
LOSS train 0.06400345332994554 valid 0.052309087956666646
EPOCH 31:
  batch 1000 loss: 0.06419301246016156
  batch 2000 loss: 0.06464411322460016
  batch 3000 loss: 0.06584135453979664
LOSS train 0.06584135453979664 valid 0.07878152553960167
EPOCH 32:
  batch 1000 loss: 0.06417525395644165
  batch 2000 loss: 0.06322937215793532
  batch 3000 loss: 0.06328038715068689
LOSS train 0.06328038715068689 valid 0.0654659929818384
EPOCH 33:
  batch 1000 loss: 0.0638205214289854
  batch 2000 loss: 0.06429229529672073
  batch 3000 loss: 0.06335471447833246
LOSS train 0.06335471447833246 valid 0.04498561188661133
EPOCH 34:
  batch 1000 loss: 0.06292893477172773
  batch 2000 loss: 0.0631494649312503
  batch 3000 loss: 0.06507624324734067
LOSS train 0.06507624324734067 valid 0.09468093203838444
EPOCH 35:
  batch 1000 loss: 0.06362017083064307
  batch 2000 loss: 0.06473764488625391
  batch 3000 loss: 0.0638669756715159
LOSS train 0.0638669756715159 valid 0.06787053031166389
EPOCH 36:
  batch 1000 loss: 0.0632307602254625
  batch 2000 loss: 0.06289113432886885
  batch 3000 loss: 0.06282824951081521
LOSS train 0.06282824951081521 valid 0.04779026615297729
EPOCH 37:
  batch 1000 loss: 0.06251160871929258
  batch 2000 loss: 0.06327172629913286
  batch 3000 loss: 0.06294296890818023
LOSS train 0.06294296890818023 valid 0.05128775990162021
EPOCH 38:
  batch 1000 loss: 0.06343285213670387
  batch 2000 loss: 0.06337033915403913
  batch 3000 loss: 0.06381915186403517
LOSS train 0.06381915186403517 valid 0.04926226597849605