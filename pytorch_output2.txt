EPOCH 1:
  batch 1000 loss: 0.18457428359322092
  batch 2000 loss: 0.10679350896637617
  batch 3000 loss: 0.09641140200989741
  batch 4000 loss: 0.08979510976859965
  batch 5000 loss: 0.08651575909328443
  batch 6000 loss: 0.08567246726926532
  batch 7000 loss: 0.08279772048875868
LOSS train 0.08279772048875868 valid 0.10646770666480733
new best vloss: 0.10646770666480733
EPOCH 2:
  batch 1000 loss: 0.08129767533705849
  batch 2000 loss: 0.07970570580011983
  batch 3000 loss: 0.07862868792827211
  batch 4000 loss: 0.07818055798283824
  batch 5000 loss: 0.07515931111133504
  batch 6000 loss: 0.0766977848048817
  batch 7000 loss: 0.07657026900634795
LOSS train 0.07657026900634795 valid 0.09672668573033055
new best vloss: 0.09672668573033055
EPOCH 3:
  batch 1000 loss: 0.07437971932220551
  batch 2000 loss: 0.07362976723850556
  batch 3000 loss: 0.0737859243294948
  batch 4000 loss: 0.07333501742728268
  batch 5000 loss: 0.0729226038244256
  batch 6000 loss: 0.07275522193607482
  batch 7000 loss: 0.07192031193240173
LOSS train 0.07192031193240173 valid 0.08390227486803875
new best vloss: 0.08390227486803875
EPOCH 4:
  batch 1000 loss: 0.07193971393674785
  batch 2000 loss: 0.07213694779936322
  batch 3000 loss: 0.07059233933521897
  batch 4000 loss: 0.07026756347474795
  batch 5000 loss: 0.07087193587263318
  batch 6000 loss: 0.06887609174804574
  batch 7000 loss: 0.07092701576883588
LOSS train 0.07092701576883588 valid 0.09028537039048387
EPOCH 5:
  batch 1000 loss: 0.06913043337894838
  batch 2000 loss: 0.06977918761573187
  batch 3000 loss: 0.06844382765004745
  batch 4000 loss: 0.0687239698837811
  batch 5000 loss: 0.069158273604522
  batch 6000 loss: 0.06830117551163725
  batch 7000 loss: 0.06923992781730184
LOSS train 0.06923992781730184 valid 0.09766410826941865
EPOCH 6:
  batch 1000 loss: 0.06827923952560508
  batch 2000 loss: 0.06772886424502206
  batch 3000 loss: 0.06776055493587221
  batch 4000 loss: 0.06936014395155302
  batch 5000 loss: 0.06744256916152028
  batch 6000 loss: 0.06757882972383408
  batch 7000 loss: 0.06772346160822046
LOSS train 0.06772346160822046 valid 0.08734768355219179
EPOCH 7:
  batch 1000 loss: 0.06751430226192554
  batch 2000 loss: 0.06674202416165988
  batch 3000 loss: 0.06684294989690578
  batch 4000 loss: 0.06746529935711817
  batch 5000 loss: 0.06733236827137876
  batch 6000 loss: 0.06720543073480516
  batch 7000 loss: 0.06670363311810269
LOSS train 0.06670363311810269 valid 0.0907158854750378
EPOCH 8:
  batch 1000 loss: 0.06550531311210234
  batch 2000 loss: 0.06741867313371523
  batch 3000 loss: 0.0664695046088049
  batch 4000 loss: 0.06611083957534568
  batch 5000 loss: 0.06609969031022032
  batch 6000 loss: 0.06560071342793639
  batch 7000 loss: 0.066058554481547
LOSS train 0.066058554481547 valid 0.08238272661292285
new best vloss: 0.08238272661292285
EPOCH 9:
  batch 1000 loss: 0.0655256659544468
  batch 2000 loss: 0.06599172143151424
  batch 3000 loss: 0.06601550935161482
  batch 4000 loss: 0.06489938311429733
  batch 5000 loss: 0.06507209226255743
  batch 6000 loss: 0.06511593288308601
  batch 7000 loss: 0.06489737322944186
LOSS train 0.06489737322944186 valid 0.09089209480007412
EPOCH 10:
  batch 1000 loss: 0.06556873055155113
  batch 2000 loss: 0.06448055735211716
  batch 3000 loss: 0.06474861785500642
  batch 4000 loss: 0.06529563692047116
  batch 5000 loss: 0.06506566158297049
  batch 6000 loss: 0.06490364843409915
  batch 7000 loss: 0.06561266721123032
LOSS train 0.06561266721123032 valid 0.09395204885398319
EPOCH 11:
  batch 1000 loss: 0.06443589543227407
  batch 2000 loss: 0.06425710380958233
  batch 3000 loss: 0.06514290640275675
  batch 4000 loss: 0.06500371640371992
  batch 5000 loss: 0.06409013838086518
  batch 6000 loss: 0.06457300449490522
  batch 7000 loss: 0.0646535730882712
LOSS train 0.0646535730882712 valid 0.095847933689159
EPOCH 12:
  batch 1000 loss: 0.06395441175433815
  batch 2000 loss: 0.06391259470210452
  batch 3000 loss: 0.06411344943627849
  batch 4000 loss: 0.06370303440630432
  batch 5000 loss: 0.06510996150239641
  batch 6000 loss: 0.06428057257753295
  batch 7000 loss: 0.06374526510722418
LOSS train 0.06374526510722418 valid 0.09076647883060407
EPOCH 13:
  batch 1000 loss: 0.06374618347408434
  batch 2000 loss: 0.0634673362564739
  batch 3000 loss: 0.06285915238937466
  batch 4000 loss: 0.0632359843500795
  batch 5000 loss: 0.06341941706622074
  batch 6000 loss: 0.0630433771335228
  batch 7000 loss: 0.06246202382367218
LOSS train 0.06246202382367218 valid 0.07524852464945676
new best vloss: 0.07524852464945676
EPOCH 14:
  batch 1000 loss: 0.06285193985329102
  batch 2000 loss: 0.0629955990661306
  batch 3000 loss: 0.0622957052154134
  batch 4000 loss: 0.06301357838570759
  batch 5000 loss: 0.06348286014418061
  batch 6000 loss: 0.06331963572646732
  batch 7000 loss: 0.06300156585813364
LOSS train 0.06300156585813364 valid 0.08370435836859542
EPOCH 15:
  batch 1000 loss: 0.06295581377106693
  batch 2000 loss: 0.062270455596083574
  batch 3000 loss: 0.06402476132786193
  batch 4000 loss: 0.06264475736201029
  batch 5000 loss: 0.06341716954063555
  batch 6000 loss: 0.06292160518750096
  batch 7000 loss: 0.06220648005590829
LOSS train 0.06220648005590829 valid 0.0781821793094423
EPOCH 16:
  batch 1000 loss: 0.06311452007148911
  batch 2000 loss: 0.061716559453844674
  batch 3000 loss: 0.062320751350631184
  batch 4000 loss: 0.06187569123231709
  batch 5000 loss: 0.06168362630212211
  batch 6000 loss: 0.061755732833779206
  batch 7000 loss: 0.063014437031585
LOSS train 0.063014437031585 valid 0.07211812491877936
new best vloss: 0.07211812491877936
EPOCH 17:
  batch 1000 loss: 0.061675235691605684
  batch 2000 loss: 0.06123325236640612
  batch 3000 loss: 0.06202819403976042
  batch 4000 loss: 0.06212184193736703
  batch 5000 loss: 0.06151197474835256
  batch 6000 loss: 0.061744509899461314
  batch 7000 loss: 0.06124351101661867
LOSS train 0.06124351101661867 valid 0.06640585392688081
new best vloss: 0.06640585392688081
EPOCH 18:
  batch 1000 loss: 0.06164075161045531
  batch 2000 loss: 0.06152572399081984
  batch 3000 loss: 0.06203906195693942
  batch 4000 loss: 0.06086908067164716
  batch 5000 loss: 0.062035295886183874
  batch 6000 loss: 0.062008570669549734
  batch 7000 loss: 0.06133517445294125
LOSS train 0.06133517445294125 valid 0.07186559969541122
EPOCH 19:
  batch 1000 loss: 0.06182449392295995
  batch 2000 loss: 0.06155099267840607
  batch 3000 loss: 0.06111471285292728
  batch 4000 loss: 0.061558851925454315
  batch 5000 loss: 0.06127726080020441
  batch 6000 loss: 0.06082307354615503
  batch 7000 loss: 0.061842290017154945
LOSS train 0.061842290017154945 valid 0.06436211874048846
new best vloss: 0.06436211874048846
EPOCH 20:
  batch 1000 loss: 0.06102809635100971
  batch 2000 loss: 0.06098481232486782
  batch 3000 loss: 0.06150940803250867
  batch 4000 loss: 0.06080408150410036
  batch 5000 loss: 0.06141390030766367
  batch 6000 loss: 0.060316778744846415
  batch 7000 loss: 0.061134891662907455
LOSS train 0.061134891662907455 valid 0.07229609339434925
EPOCH 21:
  batch 1000 loss: 0.06088642820064848
  batch 2000 loss: 0.06118067941383886
  batch 3000 loss: 0.059878589995928246
  batch 4000 loss: 0.06143157239550379
  batch 5000 loss: 0.0608718690252475
  batch 6000 loss: 0.06039473461379515
  batch 7000 loss: 0.060282754917392836
LOSS train 0.060282754917392836 valid 0.06615709245152175
EPOCH 22:
  batch 1000 loss: 0.06002117203553951
  batch 2000 loss: 0.060607999559943976
  batch 3000 loss: 0.06056065691939053
  batch 4000 loss: 0.059875245366353594
  batch 5000 loss: 0.06012030995442635
  batch 6000 loss: 0.06045716919037422
  batch 7000 loss: 0.06012438970380057
LOSS train 0.06012438970380057 valid 0.08014838863994858
EPOCH 23:
  batch 1000 loss: 0.06008443164851396
  batch 2000 loss: 0.059589648799903194
  batch 3000 loss: 0.06032342053437167
  batch 4000 loss: 0.059567716049518805
  batch 5000 loss: 0.060405446005315035
  batch 6000 loss: 0.05954617741154779
  batch 7000 loss: 0.060314361391118135
LOSS train 0.060314361391118135 valid 0.06552175003116645
EPOCH 24:
  batch 1000 loss: 0.05833023075608978
  batch 2000 loss: 0.060637252673658916
  batch 3000 loss: 0.060304087609834196
  batch 4000 loss: 0.0587892225038769
  batch 5000 loss: 0.06039155468706875
  batch 6000 loss: 0.05941198180615803
  batch 7000 loss: 0.059605935444303454
LOSS train 0.059605935444303454 valid 0.09244084057706156
EPOCH 25:
  batch 1000 loss: 0.059829410471913785
  batch 2000 loss: 0.05919050233947871
  batch 3000 loss: 0.06005538378247657
  batch 4000 loss: 0.0589212566144255
  batch 5000 loss: 0.05966057046975389
  batch 6000 loss: 0.05958708313777911
  batch 7000 loss: 0.058926580661791884
LOSS train 0.058926580661791884 valid 0.08137108902537875
EPOCH 26:
  batch 1000 loss: 0.05924607350930922
  batch 2000 loss: 0.05892860434387578
  batch 3000 loss: 0.05907505659582288
  batch 4000 loss: 0.05970426994350176
  batch 5000 loss: 0.05818390383388361
  batch 6000 loss: 0.05919333322554012
  batch 7000 loss: 0.058578958822602036
LOSS train 0.058578958822602036 valid 0.07619549733256765
EPOCH 27:
  batch 1000 loss: 0.05986452886717081
  batch 2000 loss: 0.059185499645437754
  batch 3000 loss: 0.059411427396993
  batch 4000 loss: 0.05826452916932805
  batch 5000 loss: 0.05844574434827679
  batch 6000 loss: 0.05957274432358368
  batch 7000 loss: 0.058473122706230625
LOSS train 0.058473122706230625 valid 0.06467757196332968
EPOCH 28:
  batch 1000 loss: 0.05808892400403773
  batch 2000 loss: 0.05903399990727674
  batch 3000 loss: 0.05952869316805284
  batch 4000 loss: 0.05833716964920542
  batch 5000 loss: 0.059249360067283506
  batch 6000 loss: 0.05788052107896253
  batch 7000 loss: 0.05765485358557807
LOSS train 0.05765485358557807 valid 0.06735970772163757
EPOCH 29:
  batch 1000 loss: 0.05916224927530613
  batch 2000 loss: 0.05840020168265029
  batch 3000 loss: 0.05861786451200819
  batch 4000 loss: 0.05834918902016603
  batch 5000 loss: 0.05835384184361076
  batch 6000 loss: 0.05805424946658778
  batch 7000 loss: 0.05861781054703167
LOSS train 0.05861781054703167 valid 0.0749123691864952
EPOCH 30:
  batch 1000 loss: 0.0585144319239446
  batch 2000 loss: 0.057550511261945325
  batch 3000 loss: 0.058246059979439706
  batch 4000 loss: 0.0575734696005605
  batch 5000 loss: 0.058385304760248384
  batch 6000 loss: 0.057697151641851935
  batch 7000 loss: 0.058191049998628126
LOSS train 0.058191049998628126 valid 0.06748259600056675
EPOCH 31:
  batch 1000 loss: 0.05802143214944191
  batch 2000 loss: 0.05855797967158154
  batch 3000 loss: 0.058469387928898185 (8h 19m 41s)

**************************************************************** NEW TRAINING ****************************************************************
EPOCH 1:
  batch 1000 loss: 0.05815683014774159
  batch 2000 loss: 0.057629552591360766
  batch 3000 loss: 0.05753070897748013
  batch 4000 loss: 0.05727501247760513
  batch 5000 loss: 0.05739656008506183
  batch 6000 loss: 0.05685219014792413
LOSS train 0.05685219014792413 valid 0.05185826987326436
new best vloss: 0.05185826987326436
EPOCH 2:
  batch 1000 loss: 0.05817084697690772
  batch 2000 loss: 0.0581162650993034
  batch 3000 loss: 0.05777947120939337
  batch 4000 loss: 0.05912912139747997
  batch 5000 loss: 0.057914941420396555
  batch 6000 loss: 0.057781149752973604
LOSS train 0.057781149752973604 valid 0.06443706419765173
EPOCH 3:
  batch 1000 loss: 0.057711863735607015
  batch 2000 loss: 0.0568669694547016
  batch 3000 loss: 0.0584433326503113
  batch 4000 loss: 0.058599960352777446
  batch 5000 loss: 0.05783277657453709
  batch 6000 loss: 0.058074365186019765
LOSS train 0.058074365186019765 valid 0.06554400464971574
EPOCH 4:
  batch 1000 loss: 0.05901932009909714 (22m)

**************************************************************** NEW TRAINING ****************************************************************
EPOCH 1:
  batch 1000 loss: 0.0477334669690381
LOSS train 0.0477334669690381 valid 0.049747365203681206
new best vloss: 0.049747365203681206
EPOCH 2:
  batch 1000 loss: 0.04820721046445189
LOSS train 0.04820721046445189 valid 0.0612375959385948
EPOCH 3:
  batch 1000 loss: 0.04786906209218469
LOSS train 0.04786906209218469 valid 0.04819174474517543
new best vloss: 0.04819174474517543
EPOCH 4:
  batch 1000 loss: 0.04769271970480419
LOSS train 0.04769271970480419 valid 0.05919677942923348
EPOCH 5:
  batch 1000 loss: 0.04893639963200326
LOSS train 0.04893639963200326 valid 0.048485577388661716
EPOCH 6:
  batch 1000 loss: 0.04747573084508528
LOSS train 0.04747573084508528 valid 0.05080379618727117
EPOCH 7:
  batch 1000 loss: 0.04609445784201678
LOSS train 0.04609445784201678 valid 0.04709305624952928
new best vloss: 0.04709305624952928
EPOCH 8:
  batch 1000 loss: 0.047135065097015795
LOSS train 0.047135065097015795 valid 0.04822935281990794
EPOCH 9:
  batch 1000 loss: 0.047753961415524154
LOSS train 0.047753961415524154 valid 0.03823474511336826
new best vloss: 0.03823474511336826
EPOCH 10:
  batch 1000 loss: 0.04715069822130662
LOSS train 0.04715069822130662 valid 0.03529931380071503
new best vloss: 0.03529931380071503
EPOCH 11:
  batch 1000 loss: 0.04705349012380198
LOSS train 0.04705349012380198 valid 0.037590929493065535
EPOCH 12:
  batch 1000 loss: 0.048976204034552016
LOSS train 0.048976204034552016 valid 0.04451884877125849
EPOCH 13:
  batch 1000 loss: 0.047544566578630175
LOSS train 0.047544566578630175 valid 0.05823565755029752
EPOCH 14:
  batch 1000 loss: 0.047256821912739576
LOSS train 0.047256821912739576 valid 0.06853556989917706
EPOCH 15:
  batch 1000 loss: 0.048034197634475834
LOSS train 0.048034197634475834 valid 0.05582013999349632
EPOCH 16:
  batch 1000 loss: 0.04826370304870532
LOSS train 0.04826370304870532 valid 0.05100764200845636
EPOCH 17:
  batch 1000 loss: 0.04741137327640433
LOSS train 0.04741137327640433 valid 0.04640010859056929
EPOCH 18:
  batch 1000 loss: 0.04831377405446909
LOSS train 0.04831377405446909 valid 0.047695596816265604
EPOCH 19:
  batch 1000 loss: 0.04767538399304298
LOSS train 0.04767538399304298 valid 0.04973733634784973
EPOCH 20:
  batch 1000 loss: 0.048038412876639985
LOSS train 0.048038412876639985 valid 0.0487254175915344
EPOCH 21:
  batch 1000 loss: 0.04739996950177674
LOSS train 0.04739996950177674 valid 0.05077475056229256
EPOCH 22:
  batch 1000 loss: 0.04769844014787619
LOSS train 0.04769844014787619 valid 0.05465047202609033
EPOCH 23:
  batch 1000 loss: 0.048039187679764325
LOSS train 0.048039187679764325 valid 0.05284744591117487
EPOCH 24:
  batch 1000 loss: 0.04866816000664096
LOSS train 0.04866816000664096 valid 0.058427551799832146
EPOCH 25:
  batch 1000 loss: 0.04802829959260774
LOSS train 0.04802829959260774 valid 0.05515602854793542
EPOCH 26:
  batch 1000 loss: 0.045972487814996285
LOSS train 0.045972487814996285 valid 0.0696823994027606
EPOCH 27:
  batch 1000 loss: 0.0481866681050204
LOSS train 0.0481866681050204 valid 0.05253339879612516
EPOCH 28:
  batch 1000 loss: 0.04810201926869226
LOSS train 0.04810201926869226 valid 0.05536232238373486
EPOCH 29:
  batch 1000 loss: 0.04812668709517234
LOSS train 0.04812668709517234 valid 0.06747480326994264
EPOCH 30:
  batch 1000 loss: 0.04798819689872602
LOSS train 0.04798819689872602 valid 0.05343477512712222
EPOCH 31:
  batch 1000 loss: 0.04868091594667291
LOSS train 0.04868091594667291 valid 0.05620163743224111
EPOCH 32:
  batch 1000 loss: 0.049203894443740084
LOSS train 0.049203894443740084 valid 0.051112546636431944
EPOCH 33:
  batch 1000 loss: 0.04821692449960088
LOSS train 0.04821692449960088 valid 0.05862269460606816
EPOCH 34:
  batch 1000 loss: 0.047762989447944874
LOSS train 0.047762989447944874 valid 0.057401134641258976
EPOCH 35:
  batch 1000 loss: 0.04794920861091582
LOSS train 0.04794920861091582 valid 0.05368014014408497
EPOCH 36:
  batch 1000 loss: 0.049019975390868484
LOSS train 0.049019975390868484 valid 0.06455358138021741
EPOCH 37:
  batch 1000 loss: 0.04794043877201909
LOSS train 0.04794043877201909 valid 0.055446766972211965
EPOCH 38:
  batch 1000 loss: 0.048340506851273175
LOSS train 0.048340506851273175 valid 0.07224781325409518
EPOCH 39:
  batch 1000 loss: 0.048078785757859156
LOSS train 0.048078785757859156 valid 0.05596363237661232
EPOCH 40:
  batch 1000 loss: 0.04861972730171
LOSS train 0.04861972730171 valid 0.06104277566410019
EPOCH 41:
  batch 1000 loss: 0.04820705416958587
LOSS train 0.04820705416958587 valid 0.052801068908714416
EPOCH 42:
  batch 1000 loss: 0.04783128063026269
LOSS train 0.04783128063026269 valid 0.05232496503643536 (1h 29m)

**************************************************************** NEW TRAINING ****************************************************************
EPOCH 1:
  batch 1000 loss: 0.04553678538110847
LOSS train 0.04553678538110847 valid 0.05809496095571376
new best vloss: 0.05809496095571376
EPOCH 2:
  batch 1000 loss: 0.045681929086878306
LOSS train 0.045681929086878306 valid 0.052173943023869164
new best vloss: 0.052173943023869164
EPOCH 3:
  batch 1000 loss: 0.046406218541479846
LOSS train 0.046406218541479846 valid 0.05005028650448366
new best vloss: 0.05005028650448366
EPOCH 4:
  batch 1000 loss: 0.04593033278265507
LOSS train 0.04593033278265507 valid 0.06481233941291673
EPOCH 5:
  batch 1000 loss: 0.04616097880237841
LOSS train 0.04616097880237841 valid 0.05042945761961164
EPOCH 6:
  batch 1000 loss: 0.04578278604527124
LOSS train 0.04578278604527124 valid 0.057955547685560306
EPOCH 7:
  batch 1000 loss: 0.044750511295869724
LOSS train 0.044750511295869724 valid 0.04964632373072769
new best vloss: 0.04964632373072769
EPOCH 8:
  batch 1000 loss: 0.04588205627373189
LOSS train 0.04588205627373189 valid 0.04547976680551073
new best vloss: 0.04547976680551073
EPOCH 9:
  batch 1000 loss: 0.04497425258577647
LOSS train 0.04497425258577647 valid 0.048373954117899604
EPOCH 10:
  batch 1000 loss: 0.04544811305871204
LOSS train 0.04544811305871204 valid 0.06053367080712633
EPOCH 11:
  batch 1000 loss: 0.04476123039082254
LOSS train 0.04476123039082254 valid 0.05439575397710238
EPOCH 12:
  batch 1000 loss: 0.045212774730510503
LOSS train 0.045212774730510503 valid 0.05715673770100693
EPOCH 13:
  batch 1000 loss: 0.04555154640013719
LOSS train 0.04555154640013719 valid 0.05982325333103897
EPOCH 14:
  batch 1000 loss: 0.046190569675376575
LOSS train 0.046190569675376575 valid 0.05717519590640829
EPOCH 15:
  batch 1000 loss: 0.044646875061036
LOSS train 0.044646875061036 valid 0.0566630325774895
EPOCH 16:
  batch 1000 loss: 0.044664753438760876
LOSS train 0.044664753438760876 valid 0.05344127490019067
EPOCH 17:
  batch 1000 loss: 0.043489210359734104
LOSS train 0.043489210359734104 valid 0.04856549848336726 (34m 9s)

**************************************************************** NEW TRAINING ****************************************************************
EPOCH 1:
  batch 1000 loss: 0.06654572896122517
LOSS train 0.06654572896122517 valid 0.0404654496863562
new learning rate: 0.0005
new best vloss: 0.0404654496863562
EPOCH 2:
  batch 1000 loss: 0.06560263362585517
LOSS train 0.06560263362585517 valid 0.05854752097996728
EPOCH 3:
  batch 1000 loss: 0.06375077611379315
LOSS train 0.06375077611379315 valid 0.06895772552137108
EPOCH 4:
  batch 1000 loss: 0.06392452827644723
LOSS train 0.06392452827644723 valid 0.0570243500265254
EPOCH 5:
  batch 1000 loss: 0.0631782315352343
LOSS train 0.0631782315352343 valid 0.059685463996963034
EPOCH 6:
  batch 1000 loss: 0.06307825944130482
LOSS train 0.06307825944130482 valid 0.06405856282217429
EPOCH 7:
  batch 1000 loss: 0.06406847203251217
LOSS train 0.06406847203251217 valid 0.07084946841520529
new learning rate: 0.00015
EPOCH 8:
  batch 1000 loss: 0.06057328747840331
LOSS train 0.06057328747840331 valid 0.0544331681036662
EPOCH 9:
  batch 1000 loss: 0.0586961657252134
LOSS train 0.0586961657252134 valid 0.05669031828074367
EPOCH 10:
  batch 1000 loss: 0.05816926214734731
LOSS train 0.05816926214734731 valid 0.05566094675086788
EPOCH 11:
  batch 1000 loss: 0.059092253543702664
LOSS train 0.059092253543702664 valid 0.07202704256900082
EPOCH 12:
  batch 1000 loss: 0.05922360733373858
LOSS train 0.05922360733373858 valid 0.0614233037934658
EPOCH 13:
  batch 1000 loss: 0.05934977438709038 (25m)

**************************************************************** NEW TRAINING (8h net) ****************************************************************
EPOCH 1:
  batch 1000 loss: 0.05727968146230783
LOSS train 0.05727968146230783 valid 0.06713274529126162
new learning rate: 0.0005
new best vloss: 0.06713274529126162
EPOCH 2:
  batch 1000 loss: 0.05820964719751133
LOSS train 0.05820964719751133 valid 0.06556187978700716
new best vloss: 0.06556187978700716
EPOCH 3:
  batch 1000 loss: 0.05820717433797469
LOSS train 0.05820717433797469 valid 0.062010499299200696
new best vloss: 0.062010499299200696
EPOCH 4:
  batch 1000 loss: 0.05854571992928526
LOSS train 0.05854571992928526 valid 0.09112572086669388
EPOCH 5:
  batch 1000 loss: 0.05856631857594313
LOSS train 0.05856631857594313 valid 0.08348556900697683
EPOCH 6:
  batch 1000 loss: 0.058948986869564894
LOSS train 0.058948986869564894 valid 0.05151790305462782
new best vloss: 0.05151790305462782
EPOCH 7:
  batch 1000 loss: 0.05934643197894735
LOSS train 0.05934643197894735 valid 0.08567701929508378
EPOCH 8:
  batch 1000 loss: 0.05969172988433275
LOSS train 0.05969172988433275 valid 0.05836681798415763
EPOCH 9:
  batch 1000 loss: 0.05754188030112392
LOSS train 0.05754188030112392 valid 0.07221591879788321
EPOCH 10:
  batch 1000 loss: 0.05848480632388137
LOSS train 0.05848480632388137 valid 0.06204321021723445
EPOCH 11:
  batch 1000 loss: 0.05847485664271187
LOSS train 0.05847485664271187 valid 0.05890419793504407
EPOCH 12:
  batch 1000 loss: 0.06052348902515435
LOSS train 0.06052348902515435 valid 0.07337138812496656
new learning rate: 0.00015
EPOCH 13:
  batch 1000 loss: 0.05868369654061762
LOSS train 0.05868369654061762 valid 0.06066147451019788
EPOCH 14:
  batch 1000 loss: 0.057393947683161424
LOSS train 0.057393947683161424 valid 0.05683221351452327
EPOCH 15:
  batch 1000 loss: 0.056496508048066954
LOSS train 0.056496508048066954 valid 0.059007275623177216
EPOCH 16:
  batch 1000 loss: 0.05544729564466979
LOSS train 0.05544729564466979 valid 0.05683944763416851
EPOCH 17:
  batch 1000 loss: 0.05684737794648869
LOSS train 0.05684737794648869 valid 0.062334661242978956
EPOCH 18:
  batch 1000 loss: 0.05548828884064744
LOSS train 0.05548828884064744 valid 0.054108136541738835
new learning rate: 4.4999999999999996e-05
EPOCH 19:
  batch 1000 loss: 0.05456823170695167
LOSS train 0.05456823170695167 valid 0.05329533237988168
EPOCH 20:
  batch 1000 loss: 0.05629929482773153
LOSS train 0.05629929482773153 valid 0.04986784459639845
new best vloss: 0.04986784459639845
EPOCH 21:
  batch 1000 loss: 0.05637799800126796
LOSS train 0.05637799800126796 valid 0.05521547185599047
EPOCH 22:
  batch 1000 loss: 0.05666949934838848
LOSS train 0.05666949934838848 valid 0.051814347530914046
EPOCH 23:
  batch 1000 loss: 0.0552337795031726
LOSS train 0.0552337795031726 valid 0.05467754198374072
EPOCH 24:
  batch 1000 loss: 0.053575482775582786
LOSS train 0.053575482775582786 valid 0.0502354373439933
EPOCH 25:
  batch 1000 loss: 0.054919084374722725
LOSS train 0.054919084374722725 valid 0.05304546644644385
EPOCH 26:
  batch 1000 loss: 0.053498485980738024
LOSS train 0.053498485980738024 valid 0.05459014997140912
new learning rate: 1.3499999999999998e-05
EPOCH 27:
  batch 1000 loss: 0.05398059476404045
LOSS train 0.05398059476404045 valid 0.0531530412416032
EPOCH 28:
  batch 1000 loss: 0.05534049446073259
LOSS train 0.05534049446073259 valid 0.05284733644075459
EPOCH 29:
  batch 1000 loss: 0.05430588009366672
LOSS train 0.05430588009366672 valid 0.052745407934344256
EPOCH 30:
  batch 1000 loss: 0.05505350731516737
LOSS train 0.05505350731516737 valid 0.052782842022982855
EPOCH 31:
  batch 1000 loss: 0.054948457529682686
LOSS train 0.054948457529682686 valid 0.052169479729127485
EPOCH 32:
  batch 1000 loss: 0.054040121286290564
LOSS train 0.054040121286290564 valid 0.051929874772637656
new learning rate: 4.049999999999999e-06
EPOCH 33:
  batch 1000 loss: 0.054576419863696005
LOSS train 0.054576419863696005 valid 0.05182464600620733
EPOCH 34:
  batch 1000 loss: 0.05451483966091642
LOSS train 0.05451483966091642 valid 0.053002232085418655
EPOCH 35:
  batch 1000 loss: 0.05446540477274558
LOSS train 0.05446540477274558 valid 0.05320211851018636
EPOCH 36:
  batch 1000 loss: 0.0556890027793118
LOSS train 0.0556890027793118 valid 0.053380468495136786
EPOCH 37:
  batch 1000 loss: 0.05245714740753924
LOSS train 0.05245714740753924 valid 0.05386907271543653
EPOCH 38:
  batch 1000 loss: 0.05483121608451988
LOSS train 0.05483121608451988 valid 0.05337347205277183
new learning rate: 1.2149999999999998e-06
EPOCH 39:
  batch 1000 loss: 0.05491598777044945
LOSS train 0.05491598777044945 valid 0.05420960136689246
EPOCH 40:
  batch 1000 loss: 0.054834884422579215
LOSS train 0.054834884422579215 valid 0.053668583613398366
EPOCH 41:
  batch 1000 loss: 0.05507854809071787
LOSS train 0.05507854809071787 valid 0.05430525181800476
EPOCH 42:
  batch 1000 loss: 0.05366105518263876
LOSS train 0.05366105518263876 valid 0.05413653113222002
EPOCH 43:
  batch 1000 loss: 0.05421542370187434
LOSS train 0.05421542370187434 valid 0.053525889658703814
EPOCH 44:
  batch 1000 loss: 0.05315659035755361
LOSS train 0.05315659035755361 valid 0.05353440990960128
new learning rate: 3.644999999999999e-07
EPOCH 45:
  batch 1000 loss: 0.0543745686639231
LOSS train 0.0543745686639231 valid 0.053421509456529744
EPOCH 46:
  batch 1000 loss: 0.05500299634051819
LOSS train 0.05500299634051819 valid 0.05370461421577299
EPOCH 47:
  batch 1000 loss: 0.05422968975970423
LOSS train 0.05422968975970423 valid 0.05367783434182153
EPOCH 48:
  batch 1000 loss: 0.05389981989143427
LOSS train 0.05389981989143427 valid 0.05366034677329783
EPOCH 49:
  batch 1000 loss: 0.0541007347841545
LOSS train 0.0541007347841545 valid 0.05384048154361759
EPOCH 50:
  batch 1000 loss: 0.05418562467377371
LOSS train 0.05418562467377371 valid 0.053562907189310255
new learning rate: 1.0934999999999997e-07
EPOCH 51:
  batch 1000 loss: 0.05552193319745656
LOSS train 0.05552193319745656 valid 0.053548779351694976
EPOCH 52:
  batch 1000 loss: 0.054373208058591006
LOSS train 0.054373208058591006 valid 0.0537217045022165
EPOCH 53:
  batch 1000 loss: 0.05403241718816941
LOSS train 0.05403241718816941 valid 0.05395590167997094
EPOCH 54:
  batch 1000 loss: 0.05537145829205939
LOSS train 0.05537145829205939 valid 0.05365889869911674
EPOCH 55:
  batch 1000 loss: 0.05483124777023212
LOSS train 0.05483124777023212 valid 0.053549197196844034
EPOCH 56:
  batch 1000 loss: 0.05301677964042727
LOSS train 0.05301677964042727 valid 0.05409382471674083
new learning rate: 3.280499999999999e-08
EPOCH 57:
  batch 1000 loss: 0.05453766767305974
LOSS train 0.05453766767305974 valid 0.054050989761769114
EPOCH 58:
  batch 1000 loss: 0.05528662291723226
LOSS train 0.05528662291723226 valid 0.0532432994582147
EPOCH 59:
  batch 1000 loss: 0.05391121806871725
LOSS train 0.05391121806871725 valid 0.05403852094762745
EPOCH 60:
  batch 1000 loss: 0.05496780217688586
LOSS train 0.05496780217688586 valid 0.05423995784985891
EPOCH 61:
  batch 1000 loss: 0.05466532277726377
LOSS train 0.05466532277726377 valid 0.05383798492378749
EPOCH 62:
  batch 1000 loss: 0.05382961157849427
LOSS train 0.05382961157849427 valid 0.054045691069162176
new learning rate: 9.841499999999996e-09
EPOCH 63:
  batch 1000 loss: 0.055171616188746817
LOSS train 0.055171616188746817 valid 0.05391073890884096
EPOCH 64:
  batch 1000 loss: 0.054318139773616346
LOSS train 0.054318139773616346 valid 0.05384399115573615
EPOCH 65:
  batch 1000 loss: 0.05351269448218703
LOSS train 0.05351269448218703 valid 0.05372598356140467
EPOCH 66:
  batch 1000 loss: 0.0536689527221676
LOSS train 0.0536689527221676 valid 0.05369325446336006
EPOCH 67:
  batch 1000 loss: 0.056188901583490033
LOSS train 0.056188901583490033 valid 0.05413608972424602
EPOCH 68:
  batch 1000 loss: 0.054815592440452546
LOSS train 0.054815592440452546 valid 0.05395913369829941
EPOCH 69:
  batch 1000 loss: 0.05464543148765912
LOSS train 0.05464543148765912 valid 0.05395280924203689
EPOCH 70:
  batch 1000 loss: 0.05463605497577433
LOSS train 0.05463605497577433 valid 0.053413493409001
EPOCH 71:
  batch 1000 loss: 0.054846104195285815
LOSS train 0.054846104195285815 valid 0.05390710703516864
EPOCH 72:
  batch 1000 loss: 0.05508476402399146
LOSS train 0.05508476402399146 valid 0.053115978416220365
EPOCH 73:
  batch 1000 loss: 0.05449493727410604
LOSS train 0.05449493727410604 valid 0.053872288150523674
EPOCH 74:
  batch 1000 loss: 0.05508569181497347
LOSS train 0.05508569181497347 valid 0.053901329505242755
EPOCH 75:
  batch 1000 loss: 0.05462832129473948
LOSS train 0.05462832129473948 valid 0.053856006135902135
EPOCH 76:
  batch 1000 loss: 0.05471584349663743
LOSS train 0.05471584349663743 valid 0.053812368513293525
EPOCH 77:
  batch 1000 loss: 0.05437404855910017
LOSS train 0.05437404855910017 valid 0.053724972494334604
EPOCH 78:
  batch 1000 loss: 0.05390934046773092
LOSS train 0.05390934046773092 valid 0.05362200292147463
EPOCH 79:
  batch 1000 loss: 0.054115513314357044
LOSS train 0.054115513314357044 valid 0.05399014445805127
EPOCH 80:
  batch 1000 loss: 0.05438712841742227
LOSS train 0.05438712841742227 valid 0.05378857021059957 (1h 54m 19s)

**************************************************************** NEW TRAINING (8h+1h54m net) ****************************************************************
EPOCH 1:
  batch 1000 loss: 0.059540958353023886
LOSS train 0.059540958353023886 valid 0.07143335097061936
new best vloss: 0.07143335097061936
EPOCH 2:
  batch 1000 loss: 0.06143730076086681
LOSS train 0.06143730076086681 valid 0.0771030659248936
EPOCH 3:
  batch 1000 loss: 0.05849707329542917
LOSS train 0.05849707329542917 valid 0.060949393243936355
new best vloss: 0.060949393243936355
EPOCH 4:
  batch 1000 loss: 0.06071976035482351
LOSS train 0.06071976035482351 valid 0.07740941864855509
EPOCH 5:
  batch 1000 loss: 0.058540194758898004
LOSS train 0.058540194758898004 valid 0.0713124675037155
EPOCH 6:
  batch 1000 loss: 0.06054126617147654
LOSS train 0.06054126617147654 valid 0.05631393050995636
new best vloss: 0.05631393050995636
EPOCH 7:
  batch 1000 loss: 0.058716576890731634
LOSS train 0.058716576890731634 valid 0.054697607364263
new best vloss: 0.054697607364263
EPOCH 8:
  batch 1000 loss: 0.0592706604975736
LOSS train 0.0592706604975736 valid 0.08331706764450549
EPOCH 9:
  batch 1000 loss: 0.059904721540619245
LOSS train 0.059904721540619245 valid 0.04627698776300046
new best vloss: 0.04627698776300046
EPOCH 10:
  batch 1000 loss: 0.0592404792263835
LOSS train 0.0592404792263835 valid 0.07781321568788067
EPOCH 11:
  batch 1000 loss: 0.05917166817009573
LOSS train 0.05917166817009573 valid 0.07691590198131355
EPOCH 12:
  batch 1000 loss: 0.058981002602611236
LOSS train 0.058981002602611236 valid 0.07256886191547285
EPOCH 13:
  batch 1000 loss: 0.05866656914440398
LOSS train 0.05866656914440398 valid 0.07383110626251436
new learning rate: 5e-05
EPOCH 14:
  batch 1000 loss: 0.057501207074198604
LOSS train 0.057501207074198604 valid 0.07003544277710413
EPOCH 15:
  batch 1000 loss: 0.056592838952200915
LOSS train 0.056592838952200915 valid 0.06907951364798161
EPOCH 16:
  batch 1000 loss: 0.055393611549482615
LOSS train 0.055393611549482615 valid 0.07017403721304921
EPOCH 17:
  batch 1000 loss: 0.05587126698847248
LOSS train 0.05587126698847248 valid 0.06539625020159293
new learning rate: 5e-06
EPOCH 18:
  batch 1000 loss: 0.05611153501326651
LOSS train 0.05611153501326651 valid 0.0661273072330611
EPOCH 19:
  batch 1000 loss: 0.05590418390231038
LOSS train 0.05590418390231038 valid 0.06613308884564806
EPOCH 20:
  batch 1000 loss: 0.05533318470348475
LOSS train 0.05533318470348475 valid 0.06582393760727427
EPOCH 21:
  batch 1000 loss: 0.0558588363053367
LOSS train 0.0558588363053367 valid 0.06686652390259648
new learning rate: 5.000000000000001e-07
EPOCH 22:
  batch 1000 loss: 0.05589823959446706
LOSS train 0.05589823959446706 valid 0.06809514664758656
EPOCH 23:
  batch 1000 loss: 0.055818775981315884
LOSS train 0.055818775981315884 valid 0.0676977525723487
EPOCH 24:
  batch 1000 loss: 0.05726786452186159
LOSS train 0.05726786452186159 valid 0.0667250617187771
EPOCH 25:
  batch 1000 loss: 0.05600375016255426
LOSS train 0.05600375016255426 valid 0.06717265599763778
new learning rate: 5.000000000000001e-08
EPOCH 26:
  batch 1000 loss: 0.05544813762116919
LOSS train 0.05544813762116919 valid 0.06696694357585026
EPOCH 27:
  batch 1000 loss: 0.054203350801548096
LOSS train 0.054203350801548096 valid 0.06798258182376836
EPOCH 28:
  batch 1000 loss: 0.053913805569423275
LOSS train 0.053913805569423275 valid 0.06634914840918403
EPOCH 29:
  batch 1000 loss: 0.05423035181447348
LOSS train 0.05423035181447348 valid 0.0676542284759004
new learning rate: 5.000000000000002e-09
EPOCH 30:
  batch 1000 loss: 0.056179931364157
LOSS train 0.056179931364157 valid 0.06716226597309287
EPOCH 31:
  batch 1000 loss: 0.055238823677555476
LOSS train 0.055238823677555476 valid 0.0670228803036783
EPOCH 32:
  batch 1000 loss: 0.05537778618917635
LOSS train 0.05537778618917635 valid 0.06746622194999266
EPOCH 33:
  batch 1000 loss: 0.05529172628875355
LOSS train 0.05529172628875355 valid 0.06664841899522192
EPOCH 34:
  batch 1000 loss: 0.05490143674157545
LOSS train 0.05490143674157545 valid 0.06776605361919792
EPOCH 35:
  batch 1000 loss: 0.055315441840300926
LOSS train 0.055315441840300926 valid 0.06726651302233222
EPOCH 36:
  batch 1000 loss: 0.05482560867704008
LOSS train 0.05482560867704008 valid 0.06666096210841109
EPOCH 37:
  batch 1000 loss: 0.05642609369752803
LOSS train 0.05642609369752803 valid 0.06671727265044562
EPOCH 38:
  batch 1000 loss: 0.054975589871192755
LOSS train 0.054975589871192755 valid 0.066786705557206
EPOCH 39:
  batch 1000 loss: 0.05460364060692165
LOSS train 0.05460364060692165 valid 0.06728078394577702
EPOCH 40:
  batch 1000 loss: 0.0562343683803007
LOSS train 0.0562343683803007 valid 0.06624297174312233
EPOCH 41:
  batch 1000 loss: 0.05583378025680112
LOSS train 0.05583378025680112 valid 0.06618778750131847
EPOCH 42:
  batch 1000 loss: 0.055348002828856685
LOSS train 0.055348002828856685 valid 0.06693478593151667
EPOCH 43:
  batch 1000 loss: 0.0550133353913725
LOSS train 0.0550133353913725 valid 0.06799039435651745
EPOCH 44:
  batch 1000 loss: 0.05509084950878632
LOSS train 0.05509084950878632 valid 0.06743067702858146
EPOCH 45:
  batch 1000 loss: 0.05622671782495875
LOSS train 0.05622671782495875 valid 0.06807450833742526
EPOCH 46:
  batch 1000 loss: 0.05450601357163158
LOSS train 0.05450601357163158 valid 0.06694351710430055
EPOCH 47:
  batch 1000 loss: 0.05572531612327514
LOSS train 0.05572531612327514 valid 0.06664069211061967
EPOCH 48:
  batch 1000 loss: 0.05455981064770109
LOSS train 0.05455981064770109 valid 0.06708982580312295
EPOCH 49:
  batch 1000 loss: 0.05555920336386643
LOSS train 0.05555920336386643 valid 0.06710586999518758
EPOCH 50:
  batch 1000 loss: 0.05515822458418063
LOSS train 0.05515822458418063 valid 0.06757339721092043