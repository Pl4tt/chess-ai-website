EPOCH 1:
  batch 1000 loss: 0.18457428359322092
  batch 2000 loss: 0.10679350896637617
  batch 3000 loss: 0.09641140200989741
  batch 4000 loss: 0.08979510976859965
  batch 5000 loss: 0.08651575909328443
  batch 6000 loss: 0.08567246726926532
  batch 7000 loss: 0.08279772048875868
LOSS train 0.08279772048875868 valid 0.10646770666480733
new best vloss: 0.10646770666480733
EPOCH 2:
  batch 1000 loss: 0.08129767533705849
  batch 2000 loss: 0.07970570580011983
  batch 3000 loss: 0.07862868792827211
  batch 4000 loss: 0.07818055798283824
  batch 5000 loss: 0.07515931111133504
  batch 6000 loss: 0.0766977848048817
  batch 7000 loss: 0.07657026900634795
LOSS train 0.07657026900634795 valid 0.09672668573033055
new best vloss: 0.09672668573033055
EPOCH 3:
  batch 1000 loss: 0.07437971932220551
  batch 2000 loss: 0.07362976723850556
  batch 3000 loss: 0.0737859243294948
  batch 4000 loss: 0.07333501742728268
  batch 5000 loss: 0.0729226038244256
  batch 6000 loss: 0.07275522193607482
  batch 7000 loss: 0.07192031193240173
LOSS train 0.07192031193240173 valid 0.08390227486803875
new best vloss: 0.08390227486803875
EPOCH 4:
  batch 1000 loss: 0.07193971393674785
  batch 2000 loss: 0.07213694779936322
  batch 3000 loss: 0.07059233933521897
  batch 4000 loss: 0.07026756347474795
  batch 5000 loss: 0.07087193587263318
  batch 6000 loss: 0.06887609174804574
  batch 7000 loss: 0.07092701576883588
LOSS train 0.07092701576883588 valid 0.09028537039048387
EPOCH 5:
  batch 1000 loss: 0.06913043337894838
  batch 2000 loss: 0.06977918761573187
  batch 3000 loss: 0.06844382765004745
  batch 4000 loss: 0.0687239698837811
  batch 5000 loss: 0.069158273604522
  batch 6000 loss: 0.06830117551163725
  batch 7000 loss: 0.06923992781730184
LOSS train 0.06923992781730184 valid 0.09766410826941865
EPOCH 6:
  batch 1000 loss: 0.06827923952560508
  batch 2000 loss: 0.06772886424502206
  batch 3000 loss: 0.06776055493587221
  batch 4000 loss: 0.06936014395155302
  batch 5000 loss: 0.06744256916152028
  batch 6000 loss: 0.06757882972383408
  batch 7000 loss: 0.06772346160822046
LOSS train 0.06772346160822046 valid 0.08734768355219179
EPOCH 7:
  batch 1000 loss: 0.06751430226192554
  batch 2000 loss: 0.06674202416165988
  batch 3000 loss: 0.06684294989690578
  batch 4000 loss: 0.06746529935711817
  batch 5000 loss: 0.06733236827137876
  batch 6000 loss: 0.06720543073480516
  batch 7000 loss: 0.06670363311810269
LOSS train 0.06670363311810269 valid 0.0907158854750378
EPOCH 8:
  batch 1000 loss: 0.06550531311210234
  batch 2000 loss: 0.06741867313371523
  batch 3000 loss: 0.0664695046088049
  batch 4000 loss: 0.06611083957534568
  batch 5000 loss: 0.06609969031022032
  batch 6000 loss: 0.06560071342793639
  batch 7000 loss: 0.066058554481547
LOSS train 0.066058554481547 valid 0.08238272661292285
new best vloss: 0.08238272661292285
EPOCH 9:
  batch 1000 loss: 0.0655256659544468
  batch 2000 loss: 0.06599172143151424
  batch 3000 loss: 0.06601550935161482
  batch 4000 loss: 0.06489938311429733
  batch 5000 loss: 0.06507209226255743
  batch 6000 loss: 0.06511593288308601
  batch 7000 loss: 0.06489737322944186
LOSS train 0.06489737322944186 valid 0.09089209480007412
EPOCH 10:
  batch 1000 loss: 0.06556873055155113
  batch 2000 loss: 0.06448055735211716
  batch 3000 loss: 0.06474861785500642
  batch 4000 loss: 0.06529563692047116
  batch 5000 loss: 0.06506566158297049
  batch 6000 loss: 0.06490364843409915
  batch 7000 loss: 0.06561266721123032
LOSS train 0.06561266721123032 valid 0.09395204885398319
EPOCH 11:
  batch 1000 loss: 0.06443589543227407
  batch 2000 loss: 0.06425710380958233
  batch 3000 loss: 0.06514290640275675
  batch 4000 loss: 0.06500371640371992
  batch 5000 loss: 0.06409013838086518
  batch 6000 loss: 0.06457300449490522
  batch 7000 loss: 0.0646535730882712
LOSS train 0.0646535730882712 valid 0.095847933689159
EPOCH 12:
  batch 1000 loss: 0.06395441175433815
  batch 2000 loss: 0.06391259470210452
  batch 3000 loss: 0.06411344943627849
  batch 4000 loss: 0.06370303440630432
  batch 5000 loss: 0.06510996150239641
  batch 6000 loss: 0.06428057257753295
  batch 7000 loss: 0.06374526510722418
LOSS train 0.06374526510722418 valid 0.09076647883060407
EPOCH 13:
  batch 1000 loss: 0.06374618347408434
  batch 2000 loss: 0.0634673362564739
  batch 3000 loss: 0.06285915238937466
  batch 4000 loss: 0.0632359843500795
  batch 5000 loss: 0.06341941706622074
  batch 6000 loss: 0.0630433771335228
  batch 7000 loss: 0.06246202382367218
LOSS train 0.06246202382367218 valid 0.07524852464945676
new best vloss: 0.07524852464945676
EPOCH 14:
  batch 1000 loss: 0.06285193985329102
  batch 2000 loss: 0.0629955990661306
  batch 3000 loss: 0.0622957052154134
  batch 4000 loss: 0.06301357838570759
  batch 5000 loss: 0.06348286014418061
  batch 6000 loss: 0.06331963572646732
  batch 7000 loss: 0.06300156585813364
LOSS train 0.06300156585813364 valid 0.08370435836859542
EPOCH 15:
  batch 1000 loss: 0.06295581377106693
  batch 2000 loss: 0.062270455596083574
  batch 3000 loss: 0.06402476132786193
  batch 4000 loss: 0.06264475736201029
  batch 5000 loss: 0.06341716954063555
  batch 6000 loss: 0.06292160518750096
  batch 7000 loss: 0.06220648005590829
LOSS train 0.06220648005590829 valid 0.0781821793094423
EPOCH 16:
  batch 1000 loss: 0.06311452007148911
  batch 2000 loss: 0.061716559453844674
  batch 3000 loss: 0.062320751350631184
  batch 4000 loss: 0.06187569123231709
  batch 5000 loss: 0.06168362630212211
  batch 6000 loss: 0.061755732833779206
  batch 7000 loss: 0.063014437031585
LOSS train 0.063014437031585 valid 0.07211812491877936
new best vloss: 0.07211812491877936
EPOCH 17:
  batch 1000 loss: 0.061675235691605684
  batch 2000 loss: 0.06123325236640612
  batch 3000 loss: 0.06202819403976042
  batch 4000 loss: 0.06212184193736703
  batch 5000 loss: 0.06151197474835256
  batch 6000 loss: 0.061744509899461314
  batch 7000 loss: 0.06124351101661867
LOSS train 0.06124351101661867 valid 0.06640585392688081
new best vloss: 0.06640585392688081
EPOCH 18:
  batch 1000 loss: 0.06164075161045531
  batch 2000 loss: 0.06152572399081984
  batch 3000 loss: 0.06203906195693942
  batch 4000 loss: 0.06086908067164716
  batch 5000 loss: 0.062035295886183874
  batch 6000 loss: 0.062008570669549734
  batch 7000 loss: 0.06133517445294125
LOSS train 0.06133517445294125 valid 0.07186559969541122
EPOCH 19:
  batch 1000 loss: 0.06182449392295995
  batch 2000 loss: 0.06155099267840607
  batch 3000 loss: 0.06111471285292728
  batch 4000 loss: 0.061558851925454315
  batch 5000 loss: 0.06127726080020441
  batch 6000 loss: 0.06082307354615503
  batch 7000 loss: 0.061842290017154945
LOSS train 0.061842290017154945 valid 0.06436211874048846
new best vloss: 0.06436211874048846
EPOCH 20:
  batch 1000 loss: 0.06102809635100971
  batch 2000 loss: 0.06098481232486782
  batch 3000 loss: 0.06150940803250867
  batch 4000 loss: 0.06080408150410036
  batch 5000 loss: 0.06141390030766367
  batch 6000 loss: 0.060316778744846415
  batch 7000 loss: 0.061134891662907455
LOSS train 0.061134891662907455 valid 0.07229609339434925
EPOCH 21:
  batch 1000 loss: 0.06088642820064848
  batch 2000 loss: 0.06118067941383886
  batch 3000 loss: 0.059878589995928246
  batch 4000 loss: 0.06143157239550379
  batch 5000 loss: 0.0608718690252475
  batch 6000 loss: 0.06039473461379515
  batch 7000 loss: 0.060282754917392836
LOSS train 0.060282754917392836 valid 0.06615709245152175
EPOCH 22:
  batch 1000 loss: 0.06002117203553951
  batch 2000 loss: 0.060607999559943976
  batch 3000 loss: 0.06056065691939053
  batch 4000 loss: 0.059875245366353594
  batch 5000 loss: 0.06012030995442635
  batch 6000 loss: 0.06045716919037422
  batch 7000 loss: 0.06012438970380057
LOSS train 0.06012438970380057 valid 0.08014838863994858
EPOCH 23:
  batch 1000 loss: 0.06008443164851396
  batch 2000 loss: 0.059589648799903194
  batch 3000 loss: 0.06032342053437167
  batch 4000 loss: 0.059567716049518805
  batch 5000 loss: 0.060405446005315035
  batch 6000 loss: 0.05954617741154779
  batch 7000 loss: 0.060314361391118135
LOSS train 0.060314361391118135 valid 0.06552175003116645
EPOCH 24:
  batch 1000 loss: 0.05833023075608978
  batch 2000 loss: 0.060637252673658916
  batch 3000 loss: 0.060304087609834196
  batch 4000 loss: 0.0587892225038769
  batch 5000 loss: 0.06039155468706875
  batch 6000 loss: 0.05941198180615803
  batch 7000 loss: 0.059605935444303454
LOSS train 0.059605935444303454 valid 0.09244084057706156
EPOCH 25:
  batch 1000 loss: 0.059829410471913785
  batch 2000 loss: 0.05919050233947871
  batch 3000 loss: 0.06005538378247657
  batch 4000 loss: 0.0589212566144255
  batch 5000 loss: 0.05966057046975389
  batch 6000 loss: 0.05958708313777911
  batch 7000 loss: 0.058926580661791884
LOSS train 0.058926580661791884 valid 0.08137108902537875
EPOCH 26:
  batch 1000 loss: 0.05924607350930922
  batch 2000 loss: 0.05892860434387578
  batch 3000 loss: 0.05907505659582288
  batch 4000 loss: 0.05970426994350176
  batch 5000 loss: 0.05818390383388361
  batch 6000 loss: 0.05919333322554012
  batch 7000 loss: 0.058578958822602036
LOSS train 0.058578958822602036 valid 0.07619549733256765
EPOCH 27:
  batch 1000 loss: 0.05986452886717081
  batch 2000 loss: 0.059185499645437754
  batch 3000 loss: 0.059411427396993
  batch 4000 loss: 0.05826452916932805
  batch 5000 loss: 0.05844574434827679
  batch 6000 loss: 0.05957274432358368
  batch 7000 loss: 0.058473122706230625
LOSS train 0.058473122706230625 valid 0.06467757196332968
EPOCH 28:
  batch 1000 loss: 0.05808892400403773
  batch 2000 loss: 0.05903399990727674
  batch 3000 loss: 0.05952869316805284
  batch 4000 loss: 0.05833716964920542
  batch 5000 loss: 0.059249360067283506
  batch 6000 loss: 0.05788052107896253
  batch 7000 loss: 0.05765485358557807
LOSS train 0.05765485358557807 valid 0.06735970772163757
EPOCH 29:
  batch 1000 loss: 0.05916224927530613
  batch 2000 loss: 0.05840020168265029
  batch 3000 loss: 0.05861786451200819
  batch 4000 loss: 0.05834918902016603
  batch 5000 loss: 0.05835384184361076
  batch 6000 loss: 0.05805424946658778
  batch 7000 loss: 0.05861781054703167
LOSS train 0.05861781054703167 valid 0.0749123691864952
EPOCH 30:
  batch 1000 loss: 0.0585144319239446
  batch 2000 loss: 0.057550511261945325
  batch 3000 loss: 0.058246059979439706
  batch 4000 loss: 0.0575734696005605
  batch 5000 loss: 0.058385304760248384
  batch 6000 loss: 0.057697151641851935
  batch 7000 loss: 0.058191049998628126
LOSS train 0.058191049998628126 valid 0.06748259600056675
EPOCH 1:
  batch 1000 loss: 0.05727968146230783
LOSS train 0.05727968146230783 valid 0.06713274529126162
new learning rate: 0.0005
new best vloss: 0.06713274529126162
EPOCH 2:
  batch 1000 loss: 0.05820964719751133
LOSS train 0.05820964719751133 valid 0.06556187978700716
new best vloss: 0.06556187978700716
EPOCH 3:
  batch 1000 loss: 0.05820717433797469
LOSS train 0.05820717433797469 valid 0.062010499299200696
new best vloss: 0.062010499299200696
EPOCH 4:
  batch 1000 loss: 0.05854571992928526
LOSS train 0.05854571992928526 valid 0.09112572086669388
EPOCH 5:
  batch 1000 loss: 0.05856631857594313
LOSS train 0.05856631857594313 valid 0.08348556900697683
EPOCH 6:
  batch 1000 loss: 0.058948986869564894
LOSS train 0.058948986869564894 valid 0.05151790305462782
new best vloss: 0.05151790305462782
EPOCH 7:
  batch 1000 loss: 0.05934643197894735
LOSS train 0.05934643197894735 valid 0.08567701929508378
EPOCH 8:
  batch 1000 loss: 0.05969172988433275
LOSS train 0.05969172988433275 valid 0.05836681798415763
EPOCH 9:
  batch 1000 loss: 0.05754188030112392
LOSS train 0.05754188030112392 valid 0.07221591879788321
EPOCH 10:
  batch 1000 loss: 0.05848480632388137
LOSS train 0.05848480632388137 valid 0.06204321021723445
EPOCH 11:
  batch 1000 loss: 0.05847485664271187
LOSS train 0.05847485664271187 valid 0.05890419793504407
EPOCH 12:
  batch 1000 loss: 0.06052348902515435
LOSS train 0.06052348902515435 valid 0.07337138812496656
new learning rate: 0.00015
EPOCH 13:
  batch 1000 loss: 0.05868369654061762
LOSS train 0.05868369654061762 valid 0.06066147451019788
EPOCH 14:
  batch 1000 loss: 0.057393947683161424
LOSS train 0.057393947683161424 valid 0.05683221351452327
EPOCH 15:
  batch 1000 loss: 0.056496508048066954
LOSS train 0.056496508048066954 valid 0.059007275623177216
EPOCH 16:
  batch 1000 loss: 0.05544729564466979
LOSS train 0.05544729564466979 valid 0.05683944763416851
EPOCH 17:
  batch 1000 loss: 0.05684737794648869
LOSS train 0.05684737794648869 valid 0.062334661242978956
EPOCH 18:
  batch 1000 loss: 0.05548828884064744
LOSS train 0.05548828884064744 valid 0.054108136541738835
new learning rate: 4.4999999999999996e-05
EPOCH 19:
  batch 1000 loss: 0.05456823170695167
LOSS train 0.05456823170695167 valid 0.05329533237988168
EPOCH 20:
  batch 1000 loss: 0.05629929482773153
LOSS train 0.05629929482773153 valid 0.04986784459639845
new best vloss: 0.04986784459639845
EPOCH 21:
  batch 1000 loss: 0.05637799800126796
LOSS train 0.05637799800126796 valid 0.05521547185599047
EPOCH 22:
  batch 1000 loss: 0.05666949934838848
LOSS train 0.05666949934838848 valid 0.051814347530914046
EPOCH 23:
  batch 1000 loss: 0.0552337795031726
LOSS train 0.0552337795031726 valid 0.05467754198374072
EPOCH 24:
  batch 1000 loss: 0.053575482775582786
LOSS train 0.053575482775582786 valid 0.0502354373439933
EPOCH 25:
  batch 1000 loss: 0.054919084374722725
LOSS train 0.054919084374722725 valid 0.05304546644644385
EPOCH 26:
  batch 1000 loss: 0.053498485980738024
LOSS train 0.053498485980738024 valid 0.05459014997140912
new learning rate: 1.3499999999999998e-05
EPOCH 27:
  batch 1000 loss: 0.05398059476404045
LOSS train 0.05398059476404045 valid 0.0531530412416032
EPOCH 28:
  batch 1000 loss: 0.05534049446073259
LOSS train 0.05534049446073259 valid 0.05284733644075459
EPOCH 29:
  batch 1000 loss: 0.05430588009366672
LOSS train 0.05430588009366672 valid 0.052745407934344256
EPOCH 30:
  batch 1000 loss: 0.05505350731516737
LOSS train 0.05505350731516737 valid 0.052782842022982855
EPOCH 31:
  batch 1000 loss: 0.054948457529682686
LOSS train 0.054948457529682686 valid 0.052169479729127485
EPOCH 32:
  batch 1000 loss: 0.054040121286290564
LOSS train 0.054040121286290564 valid 0.051929874772637656
new learning rate: 4.049999999999999e-06
EPOCH 33:
  batch 1000 loss: 0.054576419863696005
LOSS train 0.054576419863696005 valid 0.05182464600620733
EPOCH 34:
  batch 1000 loss: 0.05451483966091642
LOSS train 0.05451483966091642 valid 0.053002232085418655
EPOCH 35:
  batch 1000 loss: 0.05446540477274558
LOSS train 0.05446540477274558 valid 0.05320211851018636
EPOCH 36:
  batch 1000 loss: 0.0556890027793118
LOSS train 0.0556890027793118 valid 0.053380468495136786
EPOCH 37:
  batch 1000 loss: 0.05245714740753924
LOSS train 0.05245714740753924 valid 0.05386907271543653
EPOCH 38:
  batch 1000 loss: 0.05483121608451988
LOSS train 0.05483121608451988 valid 0.05337347205277183
new learning rate: 1.2149999999999998e-06
EPOCH 39:
  batch 1000 loss: 0.05491598777044945
LOSS train 0.05491598777044945 valid 0.05420960136689246
EPOCH 40:
  batch 1000 loss: 0.054834884422579215
LOSS train 0.054834884422579215 valid 0.053668583613398366
EPOCH 41:
  batch 1000 loss: 0.05507854809071787
LOSS train 0.05507854809071787 valid 0.05430525181800476
EPOCH 42:
  batch 1000 loss: 0.05366105518263876
LOSS train 0.05366105518263876 valid 0.05413653113222002
EPOCH 43:
  batch 1000 loss: 0.05421542370187434
LOSS train 0.05421542370187434 valid 0.053525889658703814
EPOCH 44:
  batch 1000 loss: 0.05315659035755361
LOSS train 0.05315659035755361 valid 0.05353440990960128
new learning rate: 3.644999999999999e-07
EPOCH 45:
  batch 1000 loss: 0.0543745686639231
LOSS train 0.0543745686639231 valid 0.053421509456529744
EPOCH 46:
  batch 1000 loss: 0.05500299634051819
LOSS train 0.05500299634051819 valid 0.05370461421577299
EPOCH 47:
  batch 1000 loss: 0.05422968975970423
LOSS train 0.05422968975970423 valid 0.05367783434182153
EPOCH 48:
  batch 1000 loss: 0.05389981989143427
LOSS train 0.05389981989143427 valid 0.05366034677329783
EPOCH 49:
  batch 1000 loss: 0.0541007347841545
LOSS train 0.0541007347841545 valid 0.05384048154361759
EPOCH 50:
  batch 1000 loss: 0.05418562467377371
LOSS train 0.05418562467377371 valid 0.053562907189310255
new learning rate: 1.0934999999999997e-07
EPOCH 51:
  batch 1000 loss: 0.05552193319745656
LOSS train 0.05552193319745656 valid 0.053548779351694976
EPOCH 52:
  batch 1000 loss: 0.054373208058591006
LOSS train 0.054373208058591006 valid 0.0537217045022165
EPOCH 53:
  batch 1000 loss: 0.05403241718816941
LOSS train 0.05403241718816941 valid 0.05395590167997094
EPOCH 54:
  batch 1000 loss: 0.05537145829205939
LOSS train 0.05537145829205939 valid 0.05365889869911674
EPOCH 55:
  batch 1000 loss: 0.05483124777023212
LOSS train 0.05483124777023212 valid 0.053549197196844034
EPOCH 56:
  batch 1000 loss: 0.05301677964042727
LOSS train 0.05301677964042727 valid 0.05409382471674083
new learning rate: 3.280499999999999e-08
EPOCH 57:
  batch 1000 loss: 0.05453766767305974
LOSS train 0.05453766767305974 valid 0.054050989761769114
EPOCH 58:
  batch 1000 loss: 0.05528662291723226
LOSS train 0.05528662291723226 valid 0.0532432994582147
EPOCH 59:
  batch 1000 loss: 0.05391121806871725
LOSS train 0.05391121806871725 valid 0.05403852094762745
EPOCH 60:
  batch 1000 loss: 0.05496780217688586
LOSS train 0.05496780217688586 valid 0.05423995784985891
EPOCH 61:
  batch 1000 loss: 0.05466532277726377
LOSS train 0.05466532277726377 valid 0.05383798492378749
EPOCH 62:
  batch 1000 loss: 0.05382961157849427
LOSS train 0.05382961157849427 valid 0.054045691069162176
new learning rate: 9.841499999999996e-09
EPOCH 63:
  batch 1000 loss: 0.055171616188746817
LOSS train 0.055171616188746817 valid 0.05391073890884096
EPOCH 64:
  batch 1000 loss: 0.054318139773616346
LOSS train 0.054318139773616346 valid 0.05384399115573615
EPOCH 65:
  batch 1000 loss: 0.05351269448218703
LOSS train 0.05351269448218703 valid 0.05372598356140467
EPOCH 66:
  batch 1000 loss: 0.0536689527221676
LOSS train 0.0536689527221676 valid 0.05369325446336006
EPOCH 67:
  batch 1000 loss: 0.056188901583490033
LOSS train 0.056188901583490033 valid 0.05413608972424602
EPOCH 68:
  batch 1000 loss: 0.054815592440452546
LOSS train 0.054815592440452546 valid 0.05395913369829941
EPOCH 69:
  batch 1000 loss: 0.05464543148765912
LOSS train 0.05464543148765912 valid 0.05395280924203689
EPOCH 70:
  batch 1000 loss: 0.05463605497577433
LOSS train 0.05463605497577433 valid 0.053413493409001
EPOCH 71:
  batch 1000 loss: 0.054846104195285815
LOSS train 0.054846104195285815 valid 0.05390710703516864
EPOCH 72:
  batch 1000 loss: 0.05508476402399146
LOSS train 0.05508476402399146 valid 0.053115978416220365
EPOCH 73:
  batch 1000 loss: 0.05449493727410604
LOSS train 0.05449493727410604 valid 0.053872288150523674
EPOCH 74:
  batch 1000 loss: 0.05508569181497347
LOSS train 0.05508569181497347 valid 0.053901329505242755
EPOCH 75:
  batch 1000 loss: 0.05462832129473948
LOSS train 0.05462832129473948 valid 0.053856006135902135
EPOCH 76:
  batch 1000 loss: 0.05471584349663743
LOSS train 0.05471584349663743 valid 0.053812368513293525
EPOCH 77:
  batch 1000 loss: 0.05437404855910017
LOSS train 0.05437404855910017 valid 0.053724972494334604
EPOCH 78:
  batch 1000 loss: 0.05390934046773092
LOSS train 0.05390934046773092 valid 0.05362200292147463
EPOCH 79:
  batch 1000 loss: 0.054115513314357044
LOSS train 0.054115513314357044 valid 0.05399014445805127
EPOCH 80:
  batch 1000 loss: 0.05438712841742227
LOSS train 0.05438712841742227 valid 0.05378857021059957